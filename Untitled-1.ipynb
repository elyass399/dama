{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00fae9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 1: Importazioni e Costanti caricate.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# --- COSTANTI DEL GIOCO ---\n",
    "VUOTO = 0\n",
    "NERO_PEDINA = 1\n",
    "NERO_DAMA = 2\n",
    "BIANCO_PEDINA = 3\n",
    "BIANCO_DAMA = 4\n",
    "DIMENSIONE = 8\n",
    "ACTION_DIM = DIMENSIONE**4  # 4096 azioni possibili\n",
    "\n",
    "print(\"Cella 1: Importazioni e Costanti caricate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de3b3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 2: Ambiente CheckersEnv definito.\n"
     ]
    }
   ],
   "source": [
    "class CheckersEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Ambiente Dama (Checkers) per Gymnasium.\n",
    "    Logica completa con movimento pedine, dame e catture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        \n",
    "        # Spazio delle Azioni: intero che rappresenta (r1, c1, r2, c2)\n",
    "        self.action_space = spaces.Discrete(DIMENSIONE**4)\n",
    "        \n",
    "        # Spazio degli Stati: Matrice 8x8\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=VUOTO, \n",
    "            high=BIANCO_DAMA, \n",
    "            shape=(DIMENSIONE, DIMENSIONE), \n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        self.player_map = {\n",
    "            NERO_PEDINA: NERO_DAMA, \n",
    "            BIANCO_PEDINA: BIANCO_DAMA\n",
    "        }\n",
    "        \n",
    "        self.board = None\n",
    "        self.current_player = None\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        \"\"\"Imposta il tabellone iniziale.\"\"\"\n",
    "        board = np.zeros((DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        # NERO (1) in alto\n",
    "        for r in range(3):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: \n",
    "                    board[r, c] = NERO_PEDINA\n",
    "        # BIANCO (3) in basso\n",
    "        for r in range(5, 8):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1:\n",
    "                    board[r, c] = BIANCO_PEDINA\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA \n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {'current_player': self.current_player}\n",
    "        \n",
    "    def render(self):\n",
    "        print(f\"\\nTurno: {'NERO' if self.current_player in [1,2] else 'BIANCO'}\")\n",
    "        print(\"   0 1 2 3 4 5 6 7\")\n",
    "        for r, row in enumerate(self.board):\n",
    "            print(f\"{r} {row}\")\n",
    "\n",
    "    # --- LOGICA MOSSE ---\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // (DIMENSIONE**3)\n",
    "        c1 = (action_int // (DIMENSIONE**2)) % DIMENSIONE\n",
    "        r2 = (action_int // DIMENSIONE) % DIMENSIONE\n",
    "        c2 = action_int % DIMENSIONE\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal_moves = []\n",
    "        king_val = self.player_map.get(self.current_player, 0)\n",
    "        \n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = self.board[r, c]\n",
    "                if piece == self.current_player or piece == king_val:\n",
    "                    captures.extend(self._get_piece_captures(r, c))\n",
    "                    if not captures:\n",
    "                        normal_moves.extend(self._get_piece_moves(r, c))\n",
    "        \n",
    "        return captures if captures else normal_moves\n",
    "\n",
    "    def _get_piece_moves(self, r, c):\n",
    "        moves = []\n",
    "        piece = self.board[r, c]\n",
    "        if piece == NERO_PEDINA: dirs = [(1, -1), (1, 1)]\n",
    "        elif piece == BIANCO_PEDINA: dirs = [(-1, -1), (-1, 1)]\n",
    "        else: dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] # Dame\n",
    "\n",
    "        for dr, dc in dirs:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < DIMENSIONE and 0 <= nc < DIMENSIONE:\n",
    "                if self.board[nr, nc] == VUOTO:\n",
    "                    moves.append((r, c, nr, nc))\n",
    "        return moves\n",
    "\n",
    "    def _get_piece_captures(self, r, c):\n",
    "        captures = []\n",
    "        piece = self.board[r, c]\n",
    "        \n",
    "        if piece in (NERO_PEDINA, NERO_DAMA):\n",
    "            enemies = (BIANCO_PEDINA, BIANCO_DAMA)\n",
    "            pawn_dirs = [(1, -1), (1, 1)]\n",
    "        else:\n",
    "            enemies = (NERO_PEDINA, NERO_DAMA)\n",
    "            pawn_dirs = [(-1, -1), (-1, 1)]\n",
    "            \n",
    "        is_king = piece in (NERO_DAMA, BIANCO_DAMA)\n",
    "        dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] if is_king else pawn_dirs\n",
    "        \n",
    "        for dr, dc in dirs:\n",
    "            mid_r, mid_c = r + dr, c + dc\n",
    "            land_r, land_c = r + 2*dr, c + 2*dc\n",
    "            \n",
    "            if 0 <= land_r < DIMENSIONE and 0 <= land_c < DIMENSIONE:\n",
    "                if self.board[mid_r, mid_c] in enemies:\n",
    "                    if self.board[land_r, land_c] == VUOTO:\n",
    "                        captures.append((r, c, land_r, land_c))\n",
    "        return captures\n",
    "\n",
    "    def _execute_move(self, r1, c1, r2, c2):\n",
    "        piece = self.board[r1, c1]\n",
    "        self.board[r2, c2] = piece\n",
    "        self.board[r1, c1] = VUOTO\n",
    "        reward = 0\n",
    "        \n",
    "        # Cattura\n",
    "        if abs(r2 - r1) == 2:\n",
    "            self.board[(r1 + r2) // 2, (c1 + c2) // 2] = VUOTO\n",
    "            reward += 1.0\n",
    "            \n",
    "        # Promozione\n",
    "        if piece == NERO_PEDINA and r2 == 7:\n",
    "            self.board[r2, c2] = NERO_DAMA\n",
    "            reward += 3.0\n",
    "        elif piece == BIANCO_PEDINA and r2 == 0:\n",
    "            self.board[r2, c2] = BIANCO_DAMA\n",
    "            reward += 3.0\n",
    "            \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        r1, c1, r2, c2 = self._decode_action(action)\n",
    "        move = (r1, c1, r2, c2)\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if move in self.possible_moves:\n",
    "            reward += self._execute_move(r1, c1, r2, c2)\n",
    "            # Cambio turno\n",
    "            self.current_player = BIANCO_PEDINA if self.current_player == NERO_PEDINA else NERO_PEDINA\n",
    "            \n",
    "            # Controllo fine gioco\n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            if not self.possible_moves:\n",
    "                terminated = True\n",
    "                reward += 10.0 # Vittoria per chi ha appena mosso\n",
    "        else:\n",
    "            reward = -10.0 # Mossa illegale\n",
    "            terminated = True \n",
    "\n",
    "        return self.board.copy(), reward, terminated, False, {}\n",
    "\n",
    "print(\"Cella 2: Ambiente CheckersEnv definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85045a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 3: Modello DQN definito.\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Rete Convoluzionale (CNN) per analizzare la scacchiera\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Dimensione dopo il flatten: 64 canali * 8 * 8\n",
    "        conv_out_size = 64 * input_shape[0] * input_shape[1]\n",
    "        \n",
    "        # Strati Fully Connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions) # Output: 4096 valori Q\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aggiunge dimensione canale: (Batch, 8, 8) -> (Batch, 1, 8, 8)\n",
    "        x = x.unsqueeze(1).float() \n",
    "        conv_out = self.conv(x)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "print(\"Cella 3: Modello DQN definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dce69e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 4: ReplayBuffer definito.\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, float(reward), done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        states = torch.tensor(np.array(batch[0]), dtype=torch.float32)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long)\n",
    "        next_states = torch.tensor(np.array(batch[2]), dtype=torch.float32)\n",
    "        rewards = torch.tensor(batch[3], dtype=torch.float32)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.bool)\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "print(\"Cella 4: ReplayBuffer definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37a61d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 5: Setup completato su cpu.\n"
     ]
    }
   ],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 500\n",
    "LR = 0.001\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = CheckersEnv()\n",
    "\n",
    "policy_net = DQN((DIMENSIONE, DIMENSIONE), ACTION_DIM).to(device)\n",
    "target_net = DQN((DIMENSIONE, DIMENSIONE), ACTION_DIM).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayBuffer(10000)\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            # Sceglie l'azione con il Q-value più alto\n",
    "            return policy_net(state_t).argmax(1).item()\n",
    "    else:\n",
    "        # Azione casuale\n",
    "        return env.action_space.sample()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    states, actions, next_states, rewards, dones = memory.sample(BATCH_SIZE)\n",
    "    states, actions, next_states, rewards, dones = states.to(device), actions.to(device), next_states.to(device), rewards.to(device), dones.to(device)\n",
    "\n",
    "    # Q(s, a)\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # V(s')\n",
    "    next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "    target_q_values = rewards + (GAMMA * next_q_values * (~dones))\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Cella 5: Setup completato su {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1291ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 6: Funzione train_agent pronta.\n"
     ]
    }
   ],
   "source": [
    "def train_agent(num_episodes):\n",
    "    print(f\"Inizio training per {num_episodes} episodi...\")\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            memory.push(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            optimize_model()\n",
    "            \n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"Episodio {i_episode}, Reward Totale: {total_reward:.1f}, Epsilon: {EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY):.2f}\")\n",
    "\n",
    "    print(\"Fine training.\")\n",
    "    torch.save(policy_net.state_dict(), \"dama_dqn_final.pth\")\n",
    "\n",
    "print(\"Cella 6: Funzione train_agent pronta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a01b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INIZIO ADDESTRAMENTO ---\n",
      "Inizio training per 100000 episodi...\n",
      "Episodio 0, Reward Totale: -10.0, Epsilon: 1.00\n",
      "Episodio 100, Reward Totale: -10.0, Epsilon: 0.99\n",
      "Episodio 200, Reward Totale: -10.0, Epsilon: 0.98\n",
      "Episodio 300, Reward Totale: -10.0, Epsilon: 0.97\n",
      "Episodio 400, Reward Totale: -10.0, Epsilon: 0.96\n",
      "Episodio 500, Reward Totale: -10.0, Epsilon: 0.95\n",
      "Episodio 600, Reward Totale: -10.0, Epsilon: 0.94\n",
      "Episodio 700, Reward Totale: -10.0, Epsilon: 0.94\n",
      "Episodio 800, Reward Totale: -10.0, Epsilon: 0.93\n",
      "Episodio 900, Reward Totale: -10.0, Epsilon: 0.92\n",
      "Episodio 1000, Reward Totale: -10.0, Epsilon: 0.91\n",
      "Episodio 1100, Reward Totale: -10.0, Epsilon: 0.90\n",
      "Episodio 1200, Reward Totale: -10.0, Epsilon: 0.89\n",
      "Episodio 1300, Reward Totale: -10.0, Epsilon: 0.88\n",
      "Episodio 1400, Reward Totale: -10.0, Epsilon: 0.87\n",
      "Episodio 1500, Reward Totale: -10.0, Epsilon: 0.86\n",
      "Episodio 1600, Reward Totale: -10.0, Epsilon: 0.85\n",
      "Episodio 1700, Reward Totale: -10.0, Epsilon: 0.85\n",
      "Episodio 1800, Reward Totale: -10.0, Epsilon: 0.84\n",
      "Episodio 1900, Reward Totale: -10.0, Epsilon: 0.83\n",
      "Episodio 2000, Reward Totale: -10.0, Epsilon: 0.82\n",
      "Episodio 2100, Reward Totale: -10.0, Epsilon: 0.81\n",
      "Episodio 2200, Reward Totale: -10.0, Epsilon: 0.80\n",
      "Episodio 2300, Reward Totale: -10.0, Epsilon: 0.79\n",
      "Episodio 2400, Reward Totale: -10.0, Epsilon: 0.78\n",
      "Episodio 2500, Reward Totale: -10.0, Epsilon: 0.77\n",
      "Episodio 2600, Reward Totale: -10.0, Epsilon: 0.77\n",
      "Episodio 2700, Reward Totale: -10.0, Epsilon: 0.76\n",
      "Episodio 2800, Reward Totale: -10.0, Epsilon: 0.75\n",
      "Episodio 2900, Reward Totale: -10.0, Epsilon: 0.74\n",
      "Episodio 3000, Reward Totale: -10.0, Epsilon: 0.73\n",
      "Episodio 3100, Reward Totale: -10.0, Epsilon: 0.72\n",
      "Episodio 3200, Reward Totale: -10.0, Epsilon: 0.71\n",
      "Episodio 3300, Reward Totale: -10.0, Epsilon: 0.71\n",
      "Episodio 3400, Reward Totale: -10.0, Epsilon: 0.70\n",
      "Episodio 3500, Reward Totale: -10.0, Epsilon: 0.69\n",
      "Episodio 3600, Reward Totale: -10.0, Epsilon: 0.68\n",
      "Episodio 3700, Reward Totale: -10.0, Epsilon: 0.67\n",
      "Episodio 3800, Reward Totale: -10.0, Epsilon: 0.66\n",
      "Episodio 3900, Reward Totale: -10.0, Epsilon: 0.66\n",
      "Episodio 4000, Reward Totale: -10.0, Epsilon: 0.65\n",
      "Episodio 4100, Reward Totale: -10.0, Epsilon: 0.64\n",
      "Episodio 4200, Reward Totale: -10.0, Epsilon: 0.63\n",
      "Episodio 4300, Reward Totale: -10.0, Epsilon: 0.62\n",
      "Episodio 4400, Reward Totale: -10.0, Epsilon: 0.62\n",
      "Episodio 4500, Reward Totale: -10.0, Epsilon: 0.61\n",
      "Episodio 4600, Reward Totale: -10.0, Epsilon: 0.60\n",
      "Episodio 4700, Reward Totale: -10.0, Epsilon: 0.59\n",
      "Episodio 4800, Reward Totale: -10.0, Epsilon: 0.58\n",
      "Episodio 4900, Reward Totale: -10.0, Epsilon: 0.58\n",
      "Episodio 5000, Reward Totale: -10.0, Epsilon: 0.57\n",
      "Episodio 5100, Reward Totale: -10.0, Epsilon: 0.56\n",
      "Episodio 5200, Reward Totale: -10.0, Epsilon: 0.55\n",
      "Episodio 5300, Reward Totale: -10.0, Epsilon: 0.55\n",
      "Episodio 5400, Reward Totale: -10.0, Epsilon: 0.54\n",
      "Episodio 5500, Reward Totale: -10.0, Epsilon: 0.53\n",
      "Episodio 5600, Reward Totale: -10.0, Epsilon: 0.53\n",
      "Episodio 5700, Reward Totale: -10.0, Epsilon: 0.52\n",
      "Episodio 5800, Reward Totale: -10.0, Epsilon: 0.51\n",
      "Episodio 5900, Reward Totale: -10.0, Epsilon: 0.50\n",
      "Episodio 6000, Reward Totale: -10.0, Epsilon: 0.50\n",
      "Episodio 6100, Reward Totale: -10.0, Epsilon: 0.49\n",
      "Episodio 6200, Reward Totale: -10.0, Epsilon: 0.49\n",
      "Episodio 6300, Reward Totale: -10.0, Epsilon: 0.48\n",
      "Episodio 6400, Reward Totale: -10.0, Epsilon: 0.47\n",
      "Episodio 6500, Reward Totale: -10.0, Epsilon: 0.47\n",
      "Episodio 6600, Reward Totale: -10.0, Epsilon: 0.46\n",
      "Episodio 6700, Reward Totale: -10.0, Epsilon: 0.45\n",
      "Episodio 6800, Reward Totale: -10.0, Epsilon: 0.45\n",
      "Episodio 6900, Reward Totale: -10.0, Epsilon: 0.44\n",
      "Episodio 7000, Reward Totale: -10.0, Epsilon: 0.44\n",
      "Episodio 7100, Reward Totale: -10.0, Epsilon: 0.43\n",
      "Episodio 7200, Reward Totale: -10.0, Epsilon: 0.43\n",
      "Episodio 7300, Reward Totale: -10.0, Epsilon: 0.42\n",
      "Episodio 7400, Reward Totale: -10.0, Epsilon: 0.41\n",
      "Episodio 7500, Reward Totale: -10.0, Epsilon: 0.41\n",
      "Episodio 7600, Reward Totale: -10.0, Epsilon: 0.40\n",
      "Episodio 7700, Reward Totale: -10.0, Epsilon: 0.40\n",
      "Episodio 7800, Reward Totale: -10.0, Epsilon: 0.39\n",
      "Episodio 7900, Reward Totale: -10.0, Epsilon: 0.39\n",
      "Episodio 8000, Reward Totale: -10.0, Epsilon: 0.38\n",
      "Episodio 8100, Reward Totale: -10.0, Epsilon: 0.38\n",
      "Episodio 8200, Reward Totale: -10.0, Epsilon: 0.37\n",
      "Episodio 8300, Reward Totale: -10.0, Epsilon: 0.37\n",
      "Episodio 8400, Reward Totale: -10.0, Epsilon: 0.36\n",
      "Episodio 8500, Reward Totale: -10.0, Epsilon: 0.36\n",
      "Episodio 8600, Reward Totale: -10.0, Epsilon: 0.35\n",
      "Episodio 8700, Reward Totale: -10.0, Epsilon: 0.35\n",
      "Episodio 8800, Reward Totale: -10.0, Epsilon: 0.34\n",
      "Episodio 8900, Reward Totale: -10.0, Epsilon: 0.34\n",
      "Episodio 9000, Reward Totale: -10.0, Epsilon: 0.33\n",
      "Episodio 9100, Reward Totale: -10.0, Epsilon: 0.33\n",
      "Episodio 9200, Reward Totale: -10.0, Epsilon: 0.32\n",
      "Episodio 9300, Reward Totale: -10.0, Epsilon: 0.32\n",
      "Episodio 9400, Reward Totale: -10.0, Epsilon: 0.32\n",
      "Episodio 9500, Reward Totale: -10.0, Epsilon: 0.31\n",
      "Episodio 9600, Reward Totale: -10.0, Epsilon: 0.31\n",
      "Episodio 9700, Reward Totale: -10.0, Epsilon: 0.30\n",
      "Episodio 9800, Reward Totale: -10.0, Epsilon: 0.30\n",
      "Episodio 9900, Reward Totale: -10.0, Epsilon: 0.30\n",
      "Episodio 10000, Reward Totale: -10.0, Epsilon: 0.29\n",
      "Episodio 10100, Reward Totale: -10.0, Epsilon: 0.29\n",
      "Episodio 10200, Reward Totale: -10.0, Epsilon: 0.28\n",
      "Episodio 10300, Reward Totale: -10.0, Epsilon: 0.28\n",
      "Episodio 10400, Reward Totale: -10.0, Epsilon: 0.27\n",
      "Episodio 10500, Reward Totale: -10.0, Epsilon: 0.27\n",
      "Episodio 10600, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 10700, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 10800, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 10900, Reward Totale: -10.0, Epsilon: 0.25\n",
      "Episodio 11000, Reward Totale: -10.0, Epsilon: 0.25\n",
      "Episodio 11100, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 11200, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 11300, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 11400, Reward Totale: -10.0, Epsilon: 0.23\n",
      "Episodio 11500, Reward Totale: -10.0, Epsilon: 0.23\n",
      "Episodio 11600, Reward Totale: -10.0, Epsilon: 0.22\n",
      "Episodio 11700, Reward Totale: -10.0, Epsilon: 0.22\n",
      "Episodio 11800, Reward Totale: -10.0, Epsilon: 0.22\n",
      "Episodio 11900, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 12000, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 12100, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 12200, Reward Totale: -10.0, Epsilon: 0.20\n",
      "Episodio 12300, Reward Totale: -10.0, Epsilon: 0.20\n",
      "Episodio 12400, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 12500, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 12600, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 12700, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 12800, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 12900, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 13000, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 13100, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 13200, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 13300, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13400, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13500, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13600, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13700, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13800, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 13900, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14000, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14100, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14200, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14300, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14400, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 14500, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 14600, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 14700, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 14800, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 14900, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15000, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15100, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15200, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15300, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15400, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15500, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 15600, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 15700, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 15800, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 15900, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 16000, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 16100, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 16200, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 16300, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16400, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16500, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16600, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16700, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16800, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 16900, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17000, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17100, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17200, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17300, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17400, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17500, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17600, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 17700, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 17800, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 17900, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18000, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18100, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18200, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18300, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18400, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18500, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18600, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 18700, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 18800, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 18900, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19000, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19100, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19200, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19300, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19400, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19500, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 19600, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 19700, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 19800, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 19900, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20000, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20100, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20200, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20300, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20400, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20500, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 20600, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 20700, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 20800, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 20900, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21000, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21100, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21200, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21300, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21400, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21500, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21600, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21700, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21800, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 21900, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 22000, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22100, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22200, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22300, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22400, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22500, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 22600, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 22700, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 22800, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 22900, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23000, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 23100, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23200, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23300, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23400, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23500, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23600, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 23700, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 23800, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 23900, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 24000, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 24100, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 24200, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 24300, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 24400, Reward Totale: -9.0, Epsilon: 0.06\n",
      "Episodio 24500, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 24600, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 24700, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 24800, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 24900, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25000, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25100, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25200, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25300, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25400, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25500, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25600, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 25700, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25800, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 25900, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 26000, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 26100, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 26200, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 26300, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 26400, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 26500, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 26600, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 26700, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 26800, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 26900, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27000, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27100, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27200, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27300, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27400, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 27500, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 27600, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 27700, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 27800, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 27900, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28000, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28100, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28200, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 28300, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28400, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28500, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 28600, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28700, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 28800, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 28900, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 29000, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29100, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 29200, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29300, Reward Totale: -9.0, Epsilon: 0.05\n",
      "Episodio 29400, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29500, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29600, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29700, Reward Totale: -10.0, Epsilon: 0.05\n",
      "Episodio 29800, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 29900, Reward Totale: -8.0, Epsilon: 0.05\n",
      "Episodio 30000, Reward Totale: -8.0, Epsilon: 0.05\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MAIN - AVVIO DEL PROGRAMMA CON ACTION MASKING\n",
    "# ==============================================================================\n",
    "\n",
    "# Funzione per convertire una mossa (r1, c1, r2, c2) in indice azione (0-4095)\n",
    "def encode_action(r1, c1, r2, c2):\n",
    "    return r1 * (DIMENSIONE**3) + c1 * (DIMENSIONE**2) + r2 * DIMENSIONE + c2\n",
    "\n",
    "# 1. ADDESTRAMENTO\n",
    "# Aumentiamo un po' gli episodi per dargli una chance di imparare qualcosa\n",
    "print(\"--- INIZIO ADDESTRAMENTO ---\")\n",
    "train_agent(num_episodes=100000) \n",
    "\n",
    "# 2. TEST PARTITA (Con Mascheramento delle azioni illegali)\n",
    "print(\"\\n--- TEST PARTITA (AI vs AI) ---\")\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "step_count = 0\n",
    "max_steps = 50 # Limitiamo la partita a 50 mosse per evitare loop infiniti\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    step_count += 1\n",
    "    \n",
    "    # 1. Ottieni le previsioni dalla rete per TUTTE le azioni\n",
    "    state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state_tensor).cpu().numpy().flatten() # Vettore di 4096 valori\n",
    "    \n",
    "    # 2. Ottieni le mosse legali dall'ambiente\n",
    "    legal_moves_tuples = env.possible_moves # Lista di (r1, c1, r2, c2)\n",
    "    \n",
    "    if not legal_moves_tuples:\n",
    "        print(\"Nessuna mossa legale disponibile. Partita terminata.\")\n",
    "        break\n",
    "        \n",
    "    # 3. ACTION MASKING: Scegli la mossa legale con il Q-value più alto\n",
    "    best_legal_action_idx = -1\n",
    "    best_legal_q_value = -float('inf')\n",
    "    \n",
    "    for move in legal_moves_tuples:\n",
    "        # Converti la mossa legale nel suo indice (0-4095)\n",
    "        r1, c1, r2, c2 = move\n",
    "        idx = encode_action(r1, c1, r2, c2)\n",
    "        \n",
    "        # Controlla se questa mossa ha un valore più alto delle altre legali trovate finora\n",
    "        if q_values[idx] > best_legal_q_value:\n",
    "            best_legal_q_value = q_values[idx]\n",
    "            best_legal_action_idx = idx\n",
    "            \n",
    "    # Se per qualche motivo non trova nulla (impossibile se la lista non è vuota), prendi la prima a caso\n",
    "    if best_legal_action_idx == -1:\n",
    "        r1, c1, r2, c2 = legal_moves_tuples[0]\n",
    "        best_legal_action_idx = encode_action(r1, c1, r2, c2)\n",
    "\n",
    "    # 4. Esegui l'azione SCELTA TRA QUELLE LEGALI\n",
    "    obs, reward, terminated, truncated, info = env.step(best_legal_action_idx)\n",
    "    env.render()\n",
    "    print(f\"Step: {step_count}, Reward: {reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        winner = \"NERO\" if reward > 0 else \"BIANCO\" # Semplificazione basata sull'ultimo reward\n",
    "        print(f\"Fine partita. Risultato finale: {reward}\")\n",
    "        break\n",
    "\n",
    "print(\"Test completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato con successo!\n",
      "ROBOT: Sposta la pedina da (5, 0) a (3, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carichiamo il cervello addestrato\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN((8, 8), 4096).to(device)\n",
    "\n",
    "# Se hai già fatto il training lungo, carica i pesi:\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"dama_dqn_final.pth\", map_location=device))\n",
    "    model.eval() # Imposta in modalità valutazione (niente apprendimento)\n",
    "    print(\"Modello caricato con successo!\")\n",
    "except:\n",
    "    print(\"Attenzione: File del modello non trovato. L'IA giocherà a caso.\")\n",
    "\n",
    "def get_best_move_for_robot(board_matrix, current_player_color):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        board_matrix: numpy array 8x8 (0=vuoto, 1=nero, etc...)\n",
    "        current_player_color: 1 (Nero) o 3 (Bianco)\n",
    "    Output:\n",
    "        ((r1, c1), (r2, c2)): Coordinate di partenza e arrivo\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Creiamo un ambiente temporaneo per calcolare le mosse legali\n",
    "    temp_env = CheckersEnv()\n",
    "    temp_env.board = board_matrix.copy()\n",
    "    temp_env.current_player = current_player_color\n",
    "    legal_moves = temp_env._find_all_legal_moves()\n",
    "    \n",
    "    if not legal_moves:\n",
    "        return None # Nessuna mossa possibile (Perso)\n",
    "\n",
    "    # 2. Chiediamo alla Rete Neurale\n",
    "    state_tensor = torch.tensor(board_matrix, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # 3. Action Masking (Scegli la migliore tra le legali)\n",
    "    best_move = None\n",
    "    best_val = -float('inf')\n",
    "\n",
    "    for move in legal_moves:\n",
    "        r1, c1, r2, c2 = move\n",
    "        # Formula inversa per ottenere l'indice da (r,c)\n",
    "        idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "        \n",
    "        if q_values[idx] > best_val:\n",
    "            best_val = q_values[idx]\n",
    "            best_move = ((r1, c1), (r2, c2))\n",
    "            \n",
    "    return best_move\n",
    "\n",
    "# --- ESEMPIO DI UTILIZZO ---\n",
    "# Immagina che la telecamera veda questa scacchiera:\n",
    "fake_camera_board = np.zeros((8,8), dtype=int)\n",
    "fake_camera_board[5, 0] = 3 # Pedina bianca in basso sinistra\n",
    "fake_camera_board[4, 1] = 1 # Pedina nera vicina (da mangiare)\n",
    "\n",
    "# Chiediamo all'IA cosa fare (Gioca il BIANCO = 3)\n",
    "mossa = get_best_move_for_robot(fake_camera_board, current_player_color=3)\n",
    "\n",
    "if mossa:\n",
    "    start, end = mossa\n",
    "    print(f\"ROBOT: Sposta la pedina da {start} a {end}\")\n",
    "else:\n",
    "    print(\"ROBOT: Non ho mosse, ho perso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dd8afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elyass Rochdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello AI caricato con successo!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 340\u001b[39m\n\u001b[32m    337\u001b[39m     sys.exit()\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m340\u001b[39m     \u001b[43mmain_gui\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 334\u001b[39m, in \u001b[36mmain_gui\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    331\u001b[39m         screen.blit(text_surf, text_rect)\n\u001b[32m    333\u001b[39m     pygame.display.flip()\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     \u001b[43mclock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m60\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m pygame.quit()\n\u001b[32m    337\u001b[39m sys.exit()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONI E COSTANTI\n",
    "# ==============================================================================\n",
    "VUOTO = 0\n",
    "NERO_PEDINA = 1\n",
    "NERO_DAMA = 2\n",
    "BIANCO_PEDINA = 3\n",
    "BIANCO_DAMA = 4\n",
    "DIMENSIONE = 8\n",
    "SQUARE_SIZE = 80 # Grandezza caselle in pixel\n",
    "WIDTH, HEIGHT = DIMENSIONE * SQUARE_SIZE, DIMENSIONE * SQUARE_SIZE\n",
    "\n",
    "# Colori\n",
    "RED = (200, 50, 50)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "BEIGE = (210, 180, 140)\n",
    "BROWN = (139, 69, 19)\n",
    "BLUE = (50, 50, 200)\n",
    "GREEN = (0, 255, 0)\n",
    "GOLD = (255, 215, 0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CLASSE AMBIENTE (CheckersEnv) - LOGICA COMPLETA\n",
    "# ==============================================================================\n",
    "class CheckersEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(DIMENSIONE**4)\n",
    "        self.observation_space = spaces.Box(low=VUOTO, high=BIANCO_DAMA, shape=(DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        \n",
    "        self.player_map = {NERO_PEDINA: NERO_DAMA, BIANCO_PEDINA: BIANCO_DAMA}\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        board = np.zeros((DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        # NERO (1)\n",
    "        for r in range(3):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: board[r, c] = NERO_PEDINA\n",
    "        # BIANCO (3)\n",
    "        for r in range(5, 8):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: board[r, c] = BIANCO_PEDINA\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA \n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {'current_player': self.current_player}\n",
    "\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // (DIMENSIONE**3)\n",
    "        c1 = (action_int // (DIMENSIONE**2)) % DIMENSIONE\n",
    "        r2 = (action_int // DIMENSIONE) % DIMENSIONE\n",
    "        c2 = action_int % DIMENSIONE\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    # --- LOGICA MOVIMENTO MANCANTE NEL TUO CODICE PRECEDENTE ---\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal_moves = []\n",
    "        king_val = self.player_map.get(self.current_player, 0)\n",
    "        \n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = self.board[r, c]\n",
    "                if piece == self.current_player or piece == king_val:\n",
    "                    captures.extend(self._get_piece_captures(r, c))\n",
    "                    if not captures:\n",
    "                        normal_moves.extend(self._get_piece_moves(r, c))\n",
    "        \n",
    "        return captures if captures else normal_moves\n",
    "\n",
    "    def _get_piece_moves(self, r, c):\n",
    "        moves = []\n",
    "        piece = self.board[r, c]\n",
    "        if piece == NERO_PEDINA: dirs = [(1, -1), (1, 1)]\n",
    "        elif piece == BIANCO_PEDINA: dirs = [(-1, -1), (-1, 1)]\n",
    "        else: dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "\n",
    "        for dr, dc in dirs:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < DIMENSIONE and 0 <= nc < DIMENSIONE:\n",
    "                if self.board[nr, nc] == VUOTO:\n",
    "                    moves.append((r, c, nr, nc))\n",
    "        return moves\n",
    "\n",
    "    def _get_piece_captures(self, r, c):\n",
    "        captures = []\n",
    "        piece = self.board[r, c]\n",
    "        \n",
    "        if piece in (NERO_PEDINA, NERO_DAMA):\n",
    "            enemies = (BIANCO_PEDINA, BIANCO_DAMA)\n",
    "            pawn_dirs = [(1, -1), (1, 1)]\n",
    "        else:\n",
    "            enemies = (NERO_PEDINA, NERO_DAMA)\n",
    "            pawn_dirs = [(-1, -1), (-1, 1)]\n",
    "            \n",
    "        is_king = piece in (NERO_DAMA, BIANCO_DAMA)\n",
    "        dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] if is_king else pawn_dirs\n",
    "        \n",
    "        for dr, dc in dirs:\n",
    "            mid_r, mid_c = r + dr, c + dc\n",
    "            land_r, land_c = r + 2*dr, c + 2*dc\n",
    "            \n",
    "            if 0 <= land_r < DIMENSIONE and 0 <= land_c < DIMENSIONE:\n",
    "                if self.board[mid_r, mid_c] in enemies:\n",
    "                    if self.board[land_r, land_c] == VUOTO:\n",
    "                        captures.append((r, c, land_r, land_c))\n",
    "        return captures\n",
    "\n",
    "    def _execute_move(self, r1, c1, r2, c2):\n",
    "        piece = self.board[r1, c1]\n",
    "        self.board[r2, c2] = piece\n",
    "        self.board[r1, c1] = VUOTO\n",
    "        reward = 0\n",
    "        \n",
    "        # Cattura\n",
    "        if abs(r2 - r1) == 2:\n",
    "            self.board[(r1 + r2) // 2, (c1 + c2) // 2] = VUOTO\n",
    "            reward += 1.0\n",
    "            \n",
    "        # Promozione\n",
    "        if piece == NERO_PEDINA and r2 == 7:\n",
    "            self.board[r2, c2] = NERO_DAMA\n",
    "            reward += 3.0\n",
    "        elif piece == BIANCO_PEDINA and r2 == 0:\n",
    "            self.board[r2, c2] = BIANCO_DAMA\n",
    "            reward += 3.0\n",
    "            \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        r1, c1, r2, c2 = self._decode_action(action)\n",
    "        move = (r1, c1, r2, c2)\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if move in self.possible_moves:\n",
    "            reward += self._execute_move(r1, c1, r2, c2)\n",
    "            # Cambio turno\n",
    "            self.current_player = BIANCO_PEDINA if self.current_player == NERO_PEDINA else NERO_PEDINA\n",
    "            \n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            if not self.possible_moves:\n",
    "                terminated = True\n",
    "                reward += 10.0 \n",
    "        else:\n",
    "            reward = -10.0 \n",
    "            terminated = True \n",
    "\n",
    "        return self.board.copy(), reward, terminated, False, {}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. RETE NEURALE (DQN)\n",
    "# ==============================================================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        conv_out_size = 64 * input_shape[0] * input_shape[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).float()\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. INTERFACCIA GRAFICA (PYGAME)\n",
    "# ==============================================================================\n",
    "def get_row_col_from_mouse(pos):\n",
    "    x, y = pos\n",
    "    row = y // SQUARE_SIZE\n",
    "    col = x // SQUARE_SIZE\n",
    "    return row, col\n",
    "\n",
    "def main_gui():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption('Dama AI vs Human')\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    env = CheckersEnv() \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN((8, 8), 8**4).to(device)\n",
    "    \n",
    "    try:\n",
    "        policy_net.load_state_dict(torch.load(\"dama_dqn_final.pth\", map_location=device))\n",
    "        policy_net.eval()\n",
    "        print(\"Modello AI caricato con successo!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ATTENZIONE: File 'dama_dqn_final.pth' non trovato.\")\n",
    "        print(\"L'IA giocherà mosse casuali (o la prima mossa legale disponibile).\")\n",
    "\n",
    "    selected_piece = None\n",
    "    valid_destinations = [] \n",
    "    \n",
    "    running = True\n",
    "    game_over = False\n",
    "    winner_text = \"\"\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "            # --- TURNO UMANO (NERO) ---\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN and not game_over:\n",
    "                if env.current_player == NERO_PEDINA:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    row, col = get_row_col_from_mouse(pos)\n",
    "                    \n",
    "                    clicked_piece = env.board[row, col]\n",
    "                    \n",
    "                    # 1. Seleziona pedina\n",
    "                    if clicked_piece in (NERO_PEDINA, NERO_DAMA):\n",
    "                        selected_piece = (row, col)\n",
    "                        valid_destinations = []\n",
    "                        for move in env.possible_moves:\n",
    "                            r1, c1, r2, c2 = move\n",
    "                            if (r1, c1) == selected_piece:\n",
    "                                valid_destinations.append((r2, c2))\n",
    "                                \n",
    "                    # 2. Muovi pedina\n",
    "                    elif selected_piece and (row, col) in valid_destinations:\n",
    "                        r1, c1 = selected_piece\n",
    "                        r2, c2 = (row, col)\n",
    "                        action_idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "                        \n",
    "                        obs, reward, terminated, truncated, info = env.step(action_idx)\n",
    "                        selected_piece = None\n",
    "                        valid_destinations = []\n",
    "                        \n",
    "                        if terminated:\n",
    "                            game_over = True\n",
    "                            winner_text = \"HAI VINTO!\"\n",
    "\n",
    "        # --- TURNO AI (BIANCO) ---\n",
    "        if env.current_player == BIANCO_PEDINA and not game_over:\n",
    "            pygame.display.set_caption(\"L'IA sta pensando...\")\n",
    "            pygame.display.flip()\n",
    "            time.sleep(0.5) \n",
    "            \n",
    "            legal_moves = env.possible_moves\n",
    "            if not legal_moves:\n",
    "                game_over = True\n",
    "                winner_text = \"HAI VINTO! (AI bloccata)\"\n",
    "            else:\n",
    "                state_tensor = torch.tensor(env.board, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state_tensor).cpu().numpy().flatten()\n",
    "                \n",
    "                best_action = -1\n",
    "                best_val = -float('inf')\n",
    "                \n",
    "                # Action Masking\n",
    "                found_move = False\n",
    "                for move in legal_moves:\n",
    "                    r1, c1, r2, c2 = move\n",
    "                    idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "                    if q_values[idx] > best_val:\n",
    "                        best_val = q_values[idx]\n",
    "                        best_action = idx\n",
    "                        found_move = True\n",
    "                \n",
    "                # Fallback se qualcosa va storto\n",
    "                if not found_move:\n",
    "                    r1, c1, r2, c2 = legal_moves[0]\n",
    "                    best_action = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(best_action)\n",
    "                \n",
    "                if terminated:\n",
    "                    game_over = True\n",
    "                    winner_text = \"HA VINTO L'IA!\"\n",
    "            \n",
    "            pygame.display.set_caption('Dama AI vs Human')\n",
    "\n",
    "        # --- DISEGNO ---\n",
    "        screen.fill(BLACK)\n",
    "        \n",
    "        # Scacchiera\n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                color = BEIGE if (r + c) % 2 == 0 else BROWN\n",
    "                pygame.draw.rect(screen, color, (c*SQUARE_SIZE, r*SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE))\n",
    "                if (r, c) in valid_destinations:\n",
    "                    pygame.draw.circle(screen, GREEN, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), 10)\n",
    "\n",
    "        # Pedine\n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = env.board[r, c]\n",
    "                if piece != 0:\n",
    "                    color = RED if piece in (1, 2) else WHITE\n",
    "                    pygame.draw.circle(screen, color, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), SQUARE_SIZE//2 - 10)\n",
    "                    if piece in (2, 4):\n",
    "                        pygame.draw.circle(screen, GOLD, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), SQUARE_SIZE//2 - 25, 3)\n",
    "                    if selected_piece == (r, c):\n",
    "                        pygame.draw.rect(screen, GREEN, (c*SQUARE_SIZE, r*SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE), 3)\n",
    "\n",
    "        if game_over:\n",
    "            font = pygame.font.SysFont(None, 75)\n",
    "            text_surf = font.render(winner_text, True, BLUE)\n",
    "            # Sfondo per il testo\n",
    "            text_rect = text_surf.get_rect(center=(WIDTH//2, HEIGHT//2))\n",
    "            pygame.draw.rect(screen, WHITE, text_rect.inflate(20, 20)) \n",
    "            screen.blit(text_surf, text_rect)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(60)\n",
    "\n",
    "    pygame.quit()\n",
    "    sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb338642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 1 completata: Classi definite.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# --- CONFIGURAZIONI ---\n",
    "DIMENSIONE = 8\n",
    "ACTION_DIM = DIMENSIONE**4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- AMBIENTE ---\n",
    "class CheckersEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(ACTION_DIM)\n",
    "        self.observation_space = spaces.Box(low=0, high=4, shape=(8,8), dtype=np.int32)\n",
    "        self.player_map = {1: 2, 3: 4}\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        board = np.zeros((8, 8), dtype=np.int32)\n",
    "        for r in range(3):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 1\n",
    "        for r in range(5, 8):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 3\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {}\n",
    "\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // 512\n",
    "        c1 = (action_int // 64) % 8\n",
    "        r2 = (action_int // 8) % 8\n",
    "        c2 = action_int % 8\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal = []\n",
    "        king = self.player_map.get(self.current_player, 0)\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if self.board[r,c] == self.current_player or self.board[r,c] == king:\n",
    "                    captures.extend(self._get_captures(r, c))\n",
    "                    if not captures: normal.extend(self._get_moves(r, c))\n",
    "        return captures if captures else normal\n",
    "\n",
    "    def _get_moves(self, r, c):\n",
    "        m = []\n",
    "        dirs = [(1,-1),(1,1)] if self.board[r,c]==1 else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs = [(1,-1),(1,1),(-1,-1),(-1,1)]\n",
    "        for dr, dc in dirs:\n",
    "            if 0<=r+dr<8 and 0<=c+dc<8 and self.board[r+dr,c+dc]==0:\n",
    "                m.append((r,c,r+dr,c+dc))\n",
    "        return m\n",
    "\n",
    "    def _get_captures(self, r, c):\n",
    "        caps = []\n",
    "        enemies = [3,4] if self.board[r,c] in [1,2] else [1,2]\n",
    "        dirs = [(1,-1),(1,1)] if self.board[r,c] in [1,2] else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs = [(1,-1),(1,1),(-1,-1),(-1,1)]\n",
    "        for dr, dc in dirs:\n",
    "            if 0<=r+2*dr<8 and 0<=c+2*dc<8:\n",
    "                if self.board[r+dr,c+dc] in enemies and self.board[r+2*dr,c+2*dc]==0:\n",
    "                    caps.append((r,c,r+2*dr,c+2*dc))\n",
    "        return caps\n",
    "\n",
    "    def step(self, action):\n",
    "        r1,c1,r2,c2 = self._decode_action(action)\n",
    "        if (r1,c1,r2,c2) in self.possible_moves:\n",
    "            self.board[r2,c2] = self.board[r1,c1]\n",
    "            self.board[r1,c1] = 0\n",
    "            rew = 0\n",
    "            if abs(r2-r1) == 2: \n",
    "                self.board[(r1+r2)//2, (c1+c2)//2] = 0\n",
    "                rew = 1.0\n",
    "            if (self.board[r2,c2]==1 and r2==7) or (self.board[r2,c2]==3 and r2==0):\n",
    "                self.board[r2,c2] += 1\n",
    "                rew += 3.0\n",
    "            \n",
    "            self.current_player = 3 if self.current_player == 1 else 1\n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            \n",
    "            done = not self.possible_moves\n",
    "            if done: rew += 10.0\n",
    "            \n",
    "            return self.board.copy(), rew, done, False, {}\n",
    "        else:\n",
    "            return self.board.copy(), -10.0, True, False, {}\n",
    "\n",
    "# --- AI ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*64, 512), nn.ReLU(),\n",
    "            nn.Linear(512, ACTION_DIM)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x.unsqueeze(1).float())\n",
    "\n",
    "print(\"Cella 1 completata: Classi definite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INIZIO ADDESTRAMENTO DOPPIO ---\n",
      "Obiettivo 1: 1.000 episodi -> 'dama_1k.pth'\n",
      "Obiettivo 2: 10.000 episodi -> 'dama_10k.pth'\n",
      "Episodio 500 completato. Epsilon: 1.00\n",
      "Episodio 1000 completato. Epsilon: 0.99\n",
      ">>> SALVATO MODELLO 1k: dama_1k.pth\n",
      "Episodio 1500 completato. Epsilon: 0.99\n",
      "Episodio 2000 completato. Epsilon: 0.98\n",
      "Episodio 2500 completato. Epsilon: 0.98\n",
      "Episodio 3000 completato. Epsilon: 0.97\n",
      "Episodio 3500 completato. Epsilon: 0.97\n",
      "Episodio 4000 completato. Epsilon: 0.96\n",
      "Episodio 4500 completato. Epsilon: 0.96\n",
      "Episodio 5000 completato. Epsilon: 0.95\n",
      "Episodio 5500 completato. Epsilon: 0.95\n",
      "Episodio 6000 completato. Epsilon: 0.94\n",
      "Episodio 6500 completato. Epsilon: 0.94\n",
      "Episodio 7000 completato. Epsilon: 0.93\n",
      "Episodio 7500 completato. Epsilon: 0.93\n",
      "Episodio 8000 completato. Epsilon: 0.92\n",
      "Episodio 8500 completato. Epsilon: 0.92\n",
      "Episodio 9000 completato. Epsilon: 0.92\n",
      "Episodio 9500 completato. Epsilon: 0.91\n",
      "Episodio 10000 completato. Epsilon: 0.91\n",
      ">>> SALVATO MODELLO 10k: dama_10k.pth\n",
      "Addestramento completato.\n"
     ]
    }
   ],
   "source": [
    "# Setup Training\n",
    "env = CheckersEnv()\n",
    "policy_net = DQN().to(DEVICE)\n",
    "target_net = DQN().to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = deque(maxlen=20000)\n",
    "\n",
    "steps_done = 0\n",
    "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 100000\n",
    "\n",
    "print(\"--- INIZIO ADDESTRAMENTO DOPPIO ---\")\n",
    "print(\"Obiettivo 1: 1.000 episodi -> 'dama_1k.pth'\")\n",
    "print(\"Obiettivo 2: 10.000 episodi -> 'dama_10k.pth'\")\n",
    "\n",
    "for i_episode in range(1, 10001):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        steps_done += 1\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "        # Select Action\n",
    "        if random.random() > eps:\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.tensor(state).unsqueeze(0).to(DEVICE)\n",
    "                action = policy_net(state_t).argmax().item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Step\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        \n",
    "        memory.append((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Optimize\n",
    "        if len(memory) > 64:\n",
    "            batch = random.sample(memory, 64)\n",
    "            states, acts, nexts, rews, dones = zip(*batch)\n",
    "            \n",
    "            S = torch.tensor(np.array(states)).to(DEVICE)\n",
    "            A = torch.tensor(acts).unsqueeze(1).to(DEVICE)\n",
    "            R = torch.tensor(rews).to(DEVICE)\n",
    "            NS = torch.tensor(np.array(nexts)).to(DEVICE)\n",
    "            D = torch.tensor(dones).to(DEVICE)\n",
    "            \n",
    "            Q = policy_net(S).gather(1, A).squeeze()\n",
    "            next_Q = target_net(NS).max(1)[0].detach()\n",
    "            Target = R + 0.99 * next_Q * (~D)\n",
    "            \n",
    "            loss = nn.MSELoss()(Q, Target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Update Target Network\n",
    "    if i_episode % 500 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episodio {i_episode} completato. Epsilon: {eps:.2f}\")\n",
    "\n",
    "    # SALVATAGGIO STEP 1 (1.000 Episodi)\n",
    "    if i_episode == 1000:\n",
    "        torch.save(policy_net.state_dict(), \"dama_1k.pth\")\n",
    "        print(\">>> SALVATO MODELLO 1k: dama_1k.pth\")\n",
    "\n",
    "    # SALVATAGGIO STEP 2 (10.000 Episodi)\n",
    "    if i_episode == 10000:\n",
    "        torch.save(policy_net.state_dict(), \"dama_10k.pth\")\n",
    "        print(\">>> SALVATO MODELLO 10k: dama_10k.pth\")\n",
    "\n",
    "print(\"Addestramento completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619b409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Elyass Rochdi\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.13.9)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for DQN:\n\tMissing key(s) in state_dict: \"conv.0.weight\", \"conv.0.bias\", \"conv.2.weight\", \"conv.2.bias\", \"fc.0.weight\", \"fc.0.bias\", \"fc.2.weight\", \"fc.2.bias\". \n\tUnexpected key(s) in state_dict: \"net.0.weight\", \"net.0.bias\", \"net.2.weight\", \"net.2.bias\", \"net.5.weight\", \"net.5.bias\", \"net.7.weight\", \"net.7.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 208\u001b[39m\n\u001b[32m    205\u001b[39m         pygame.display.flip()\n\u001b[32m    206\u001b[39m         clock.tick(\u001b[32m30\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m208\u001b[39m \u001b[43mai_vs_ai_showdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mai_vs_ai_showdown\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    132\u001b[39m net_beginner.eval()\n\u001b[32m    134\u001b[39m net_expert = DQN().to(device)\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m \u001b[43mnet_expert\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_ESPERTO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m net_expert.eval()\n\u001b[32m    138\u001b[39m env = CheckersEnv()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\nn\\modules\\module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for DQN:\n\tMissing key(s) in state_dict: \"conv.0.weight\", \"conv.0.bias\", \"conv.2.weight\", \"conv.2.bias\", \"fc.0.weight\", \"fc.0.bias\", \"fc.2.weight\", \"fc.2.bias\". \n\tUnexpected key(s) in state_dict: \"net.0.weight\", \"net.0.bias\", \"net.2.weight\", \"net.2.bias\", \"net.5.weight\", \"net.5.bias\", \"net.7.weight\", \"net.7.bias\". "
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURAZIONE E CLASSI COMPATIBILI (FIX PER IL TUO ERRORE)\n",
    "# ==============================================================================\n",
    "\n",
    "DIMENSIONE = 8\n",
    "ACTION_DIM = DIMENSIONE**4\n",
    "\n",
    "class CheckersEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(ACTION_DIM)\n",
    "        self.observation_space = spaces.Box(low=0, high=4, shape=(8,8), dtype=np.int32)\n",
    "        self.player_map = {1: 2, 3: 4}\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = []\n",
    "    def _initialize_board(self):\n",
    "        board = np.zeros((8, 8), dtype=np.int32)\n",
    "        for r in range(3):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 1\n",
    "        for r in range(5, 8):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 3\n",
    "        return board\n",
    "    def reset(self, seed=None):\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {}\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // 512; c1 = (action_int // 64) % 8\n",
    "        r2 = (action_int // 8) % 8; c2 = action_int % 8\n",
    "        return r1, c1, r2, c2\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures, normal = [], []\n",
    "        king = self.player_map.get(self.current_player, 0)\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if self.board[r,c]==self.current_player or self.board[r,c]==king:\n",
    "                    captures.extend(self._get_captures(r,c))\n",
    "                    if not captures: normal.extend(self._get_moves(r,c))\n",
    "        return captures if captures else normal\n",
    "    def _get_moves(self, r, c):\n",
    "        m=[]; dirs=[(1,-1),(1,1)] if self.board[r,c]==1 else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs=[(1,-1),(1,1),(-1,-1),(-1,1)]\n",
    "        for dr,dc in dirs:\n",
    "            if 0<=r+dr<8 and 0<=c+dc<8 and self.board[r+dr,c+dc]==0: m.append((r,c,r+dr,c+dc))\n",
    "        return m\n",
    "    def _get_captures(self, r, c):\n",
    "        caps=[]; en=[3,4] if self.board[r,c] in [1,2] else [1,2]\n",
    "        dirs=[(1,-1),(1,1)] if self.board[r,c] in [1,2] else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs=[(-1,-1),(-1,1),(1,-1),(1,1)]\n",
    "        for dr,dc in dirs:\n",
    "            if 0<=r+2*dr<8 and 0<=c+2*dc<8:\n",
    "                if self.board[r+dr,c+dc] in en and self.board[r+2*dr,c+2*dc]==0: caps.append((r,c,r+2*dr,c+2*dc))\n",
    "        return caps\n",
    "    def step(self, action):\n",
    "        r1,c1,r2,c2=self._decode_action(action)\n",
    "        if (r1,c1,r2,c2) in self.possible_moves:\n",
    "            self.board[r2,c2]=self.board[r1,c1]; self.board[r1,c1]=0\n",
    "            if abs(r2-r1)==2: self.board[(r1+r2)//2,(c1+c2)//2]=0\n",
    "            if (self.board[r2,c2]==1 and r2==7) or (self.board[r2,c2]==3 and r2==0): self.board[r2,c2]+=1\n",
    "            self.current_player=3 if self.current_player==1 else 1\n",
    "            self.possible_moves=self._find_all_legal_moves()\n",
    "            return self.board.copy(),0,not self.possible_moves,False,{}\n",
    "        return self.board,-10,True,False,{}\n",
    "\n",
    "# --- QUESTA È LA CLASSE CORRETTA PER I TUOI FILE VECCHI ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # Struttura divisa in 'conv' e 'fc' come nel tuo file salvato\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        conv_out_size = 64 * 8 * 8\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, ACTION_DIM)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).float() \n",
    "        conv_out = self.conv(x)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "# ==============================================================================\n",
    "# SCONTRO AI vs AI\n",
    "# ==============================================================================\n",
    "\n",
    "# Assicurati che questi nomi siano ESATTI rispetto a quelli che hai nella cartella\n",
    "FILE_PRINCIPIANTE = \"dama_dqn_final.pth\" # Quello da 1k episodi (o 500)\n",
    "FILE_ESPERTO = \"dama_10k.pth\"            # Quello da 10k episodi\n",
    "\n",
    "SQUARE_SIZE = 80\n",
    "WIDTH, HEIGHT = 640, 640\n",
    "COLORS = {'W':(255,255,255), 'B':(0,0,0), 'R':(200,50,50), 'Beige':(210,180,140), 'Brown':(139,69,19), 'Green':(0,255,0), 'Gold':(255,215,0)}\n",
    "\n",
    "def ai_vs_ai_showdown():\n",
    "    if not os.path.exists(FILE_PRINCIPIANTE) or not os.path.exists(FILE_ESPERTO):\n",
    "        print(\"ERRORE: Uno dei due file non esiste nella cartella.\")\n",
    "        print(f\"Cerco: {FILE_PRINCIPIANTE} e {FILE_ESPERTO}\")\n",
    "        return\n",
    "\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(f'AI Battle: {FILE_PRINCIPIANTE} vs {FILE_ESPERTO}')\n",
    "    clock = pygame.time.Clock()\n",
    "    font = pygame.font.SysFont(None, 40)\n",
    "\n",
    "    device = torch.device(\"cpu\")\n",
    "    \n",
    "    # Caricamento Modelli\n",
    "    net_beginner = DQN().to(device)\n",
    "    net_beginner.load_state_dict(torch.load(FILE_PRINCIPIANTE, map_location=device))\n",
    "    net_beginner.eval()\n",
    "    \n",
    "    net_expert = DQN().to(device)\n",
    "    net_expert.load_state_dict(torch.load(FILE_ESPERTO, map_location=device))\n",
    "    net_expert.eval()\n",
    "\n",
    "    env = CheckersEnv()\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    running = True\n",
    "    game_over = False\n",
    "    winner_text = \"\"\n",
    "    \n",
    "    print(\"Inizio partita...\")\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                return\n",
    "\n",
    "        if not game_over:\n",
    "            pygame.display.flip()\n",
    "            time.sleep(0.5) # Ritardo per vedere le mosse\n",
    "\n",
    "            legal_moves = env.possible_moves\n",
    "            if not legal_moves:\n",
    "                game_over = True\n",
    "                winner_text = \"Vince BIANCO (Stallo)\" if env.current_player == 1 else \"Vince NERO (Stallo)\"\n",
    "            else:\n",
    "                board_tensor = torch.tensor(env.board).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Scelta Rete in base al giocatore\n",
    "                if env.current_player == 1: # Nero\n",
    "                    with torch.no_grad(): q_values = net_beginner(board_tensor).flatten()\n",
    "                else: # Bianco\n",
    "                    with torch.no_grad(): q_values = net_expert(board_tensor).flatten()\n",
    "\n",
    "                # Action Masking\n",
    "                best_idx = -1\n",
    "                best_val = -float('inf')\n",
    "                for (r1, c1, r2, c2) in legal_moves:\n",
    "                    idx = r1*512 + c1*64 + r2*8 + c2\n",
    "                    if q_values[idx] > best_val:\n",
    "                        best_val = q_values[idx]\n",
    "                        best_idx = idx\n",
    "                \n",
    "                obs, _, term, _, _ = env.step(best_idx)\n",
    "                if term:\n",
    "                    game_over = True\n",
    "                    winner_text = \"Vince BIANCO (Esperto)!\" if env.current_player == 1 else \"Vince NERO (Principiante)!\"\n",
    "\n",
    "        # Disegno\n",
    "        screen.fill(COLORS['B'])\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                col = COLORS['Beige'] if (r+c)%2==0 else COLORS['Brown']\n",
    "                pygame.draw.rect(screen, col, (c*80, r*80, 80, 80))\n",
    "                p = env.board[r,c]\n",
    "                if p != 0:\n",
    "                    cc = COLORS['R'] if p in [1,2] else COLORS['W']\n",
    "                    pygame.draw.circle(screen, cc, (c*80+40, r*80+40), 30)\n",
    "                    if p in [2,4]: pygame.draw.circle(screen, COLORS['Gold'], (c*80+40, r*80+40), 10)\n",
    "\n",
    "        if game_over:\n",
    "            s = pygame.Surface((WIDTH, HEIGHT), pygame.SRCALPHA)\n",
    "            s.fill((0,0,0,180))\n",
    "            screen.blit(s, (0,0))\n",
    "            txt = font.render(winner_text, True, COLORS['Green'])\n",
    "            tr = txt.get_rect(center=(WIDTH/2, HEIGHT/2))\n",
    "            screen.blit(txt, tr)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(30)\n",
    "\n",
    "ai_vs_ai_showdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
