{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "00fae9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 1: Importazioni e Costanti caricate.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "import sys\n",
    "\n",
    "# --- COSTANTI DEL GIOCO ---\n",
    "VUOTO = 0\n",
    "NERO_PEDINA = 1\n",
    "NERO_DAMA = 2\n",
    "BIANCO_PEDINA = 3\n",
    "BIANCO_DAMA = 4\n",
    "DIMENSIONE = 8\n",
    "ACTION_DIM = DIMENSIONE**4  # 4096 azioni possibili\n",
    "\n",
    "print(\"Cella 1: Importazioni e Costanti caricate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de3b3ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 2: Ambiente CheckersEnv definito.\n"
     ]
    }
   ],
   "source": [
    "class CheckersEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Ambiente Dama (Checkers) per Gymnasium.\n",
    "    Logica completa con movimento pedine, dame e catture.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        \n",
    "        # Spazio delle Azioni: intero che rappresenta (r1, c1, r2, c2)\n",
    "        self.action_space = spaces.Discrete(DIMENSIONE**4)\n",
    "        \n",
    "        # Spazio degli Stati: Matrice 8x8\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=VUOTO, \n",
    "            high=BIANCO_DAMA, \n",
    "            shape=(DIMENSIONE, DIMENSIONE), \n",
    "            dtype=np.int32\n",
    "        )\n",
    "        \n",
    "        self.player_map = {\n",
    "            NERO_PEDINA: NERO_DAMA, \n",
    "            BIANCO_PEDINA: BIANCO_DAMA\n",
    "        }\n",
    "        \n",
    "        self.board = None\n",
    "        self.current_player = None\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        \"\"\"Imposta il tabellone iniziale.\"\"\"\n",
    "        board = np.zeros((DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        # NERO (1) in alto\n",
    "        for r in range(3):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: \n",
    "                    board[r, c] = NERO_PEDINA\n",
    "        # BIANCO (3) in basso\n",
    "        for r in range(5, 8):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1:\n",
    "                    board[r, c] = BIANCO_PEDINA\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA \n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {'current_player': self.current_player}\n",
    "        \n",
    "    def render(self):\n",
    "        print(f\"\\nTurno: {'NERO' if self.current_player in [1,2] else 'BIANCO'}\")\n",
    "        print(\"   0 1 2 3 4 5 6 7\")\n",
    "        for r, row in enumerate(self.board):\n",
    "            print(f\"{r} {row}\")\n",
    "\n",
    "    # --- LOGICA MOSSE ---\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // (DIMENSIONE**3)\n",
    "        c1 = (action_int // (DIMENSIONE**2)) % DIMENSIONE\n",
    "        r2 = (action_int // DIMENSIONE) % DIMENSIONE\n",
    "        c2 = action_int % DIMENSIONE\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal_moves = []\n",
    "        king_val = self.player_map.get(self.current_player, 0)\n",
    "        \n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = self.board[r, c]\n",
    "                if piece == self.current_player or piece == king_val:\n",
    "                    captures.extend(self._get_piece_captures(r, c))\n",
    "                    if not captures:\n",
    "                        normal_moves.extend(self._get_piece_moves(r, c))\n",
    "        \n",
    "        return captures if captures else normal_moves\n",
    "\n",
    "    def _get_piece_moves(self, r, c):\n",
    "        moves = []\n",
    "        piece = self.board[r, c]\n",
    "        if piece == NERO_PEDINA: dirs = [(1, -1), (1, 1)]\n",
    "        elif piece == BIANCO_PEDINA: dirs = [(-1, -1), (-1, 1)]\n",
    "        else: dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] # Dame\n",
    "\n",
    "        for dr, dc in dirs:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < DIMENSIONE and 0 <= nc < DIMENSIONE:\n",
    "                if self.board[nr, nc] == VUOTO:\n",
    "                    moves.append((r, c, nr, nc))\n",
    "        return moves\n",
    "\n",
    "    def _get_piece_captures(self, r, c):\n",
    "        captures = []\n",
    "        piece = self.board[r, c]\n",
    "        \n",
    "        if piece in (NERO_PEDINA, NERO_DAMA):\n",
    "            enemies = (BIANCO_PEDINA, BIANCO_DAMA)\n",
    "            pawn_dirs = [(1, -1), (1, 1)]\n",
    "        else:\n",
    "            enemies = (NERO_PEDINA, NERO_DAMA)\n",
    "            pawn_dirs = [(-1, -1), (-1, 1)]\n",
    "            \n",
    "        is_king = piece in (NERO_DAMA, BIANCO_DAMA)\n",
    "        dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] if is_king else pawn_dirs\n",
    "        \n",
    "        for dr, dc in dirs:\n",
    "            mid_r, mid_c = r + dr, c + dc\n",
    "            land_r, land_c = r + 2*dr, c + 2*dc\n",
    "            \n",
    "            if 0 <= land_r < DIMENSIONE and 0 <= land_c < DIMENSIONE:\n",
    "                if self.board[mid_r, mid_c] in enemies:\n",
    "                    if self.board[land_r, land_c] == VUOTO:\n",
    "                        captures.append((r, c, land_r, land_c))\n",
    "        return captures\n",
    "\n",
    "    def _execute_move(self, r1, c1, r2, c2):\n",
    "        piece = self.board[r1, c1]\n",
    "        self.board[r2, c2] = piece\n",
    "        self.board[r1, c1] = VUOTO\n",
    "        reward = 0\n",
    "        \n",
    "        # Cattura\n",
    "        if abs(r2 - r1) == 2:\n",
    "            self.board[(r1 + r2) // 2, (c1 + c2) // 2] = VUOTO\n",
    "            reward += 1.0\n",
    "            \n",
    "        # Promozione\n",
    "        if piece == NERO_PEDINA and r2 == 7:\n",
    "            self.board[r2, c2] = NERO_DAMA\n",
    "            reward += 3.0\n",
    "        elif piece == BIANCO_PEDINA and r2 == 0:\n",
    "            self.board[r2, c2] = BIANCO_DAMA\n",
    "            reward += 3.0\n",
    "            \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        r1, c1, r2, c2 = self._decode_action(action)\n",
    "        move = (r1, c1, r2, c2)\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if move in self.possible_moves:\n",
    "            reward += self._execute_move(r1, c1, r2, c2)\n",
    "            # Cambio turno\n",
    "            self.current_player = BIANCO_PEDINA if self.current_player == NERO_PEDINA else NERO_PEDINA\n",
    "            \n",
    "            # Controllo fine gioco\n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            if not self.possible_moves:\n",
    "                terminated = True\n",
    "                reward += 10.0 # Vittoria per chi ha appena mosso\n",
    "        else:\n",
    "            reward = -10.0 # Mossa illegale\n",
    "            terminated = True \n",
    "\n",
    "        return self.board.copy(), reward, terminated, False, {}\n",
    "\n",
    "print(\"Cella 2: Ambiente CheckersEnv definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85045a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 3: Modello DQN definito.\n"
     ]
    }
   ],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # Rete Convoluzionale (CNN) per analizzare la scacchiera\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Dimensione dopo il flatten: 64 canali * 8 * 8\n",
    "        conv_out_size = 64 * input_shape[0] * input_shape[1]\n",
    "        \n",
    "        # Strati Fully Connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions) # Output: 4096 valori Q\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Aggiunge dimensione canale: (Batch, 8, 8) -> (Batch, 1, 8, 8)\n",
    "        x = x.unsqueeze(1).float() \n",
    "        conv_out = self.conv(x)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "print(\"Cella 3: Modello DQN definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1dce69e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 4: ReplayBuffer definito.\n"
     ]
    }
   ],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, next_state, reward, done):\n",
    "        self.memory.append((state, action, next_state, float(reward), done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.memory, batch_size)\n",
    "        batch = list(zip(*transitions))\n",
    "        \n",
    "        states = torch.tensor(np.array(batch[0]), dtype=torch.float32)\n",
    "        actions = torch.tensor(batch[1], dtype=torch.long)\n",
    "        next_states = torch.tensor(np.array(batch[2]), dtype=torch.float32)\n",
    "        rewards = torch.tensor(batch[3], dtype=torch.float32)\n",
    "        dones = torch.tensor(batch[4], dtype=torch.bool)\n",
    "        \n",
    "        return states, actions, next_states, rewards, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "print(\"Cella 4: ReplayBuffer definito.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "37a61d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 5: Setup completato su cpu.\n"
     ]
    }
   ],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 10000\n",
    "TARGET_UPDATE = 500\n",
    "LR = 0.001\n",
    "\n",
    "# --- SETUP ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "env = CheckersEnv()\n",
    "\n",
    "policy_net = DQN((DIMENSIONE, DIMENSIONE), ACTION_DIM).to(device)\n",
    "target_net = DQN((DIMENSIONE, DIMENSIONE), ACTION_DIM).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n",
    "memory = ReplayBuffer(10000)\n",
    "steps_done = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    \n",
    "    if random.random() > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            state_t = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            # Sceglie l'azione con il Q-value più alto\n",
    "            return policy_net(state_t).argmax(1).item()\n",
    "    else:\n",
    "        # Azione casuale\n",
    "        return env.action_space.sample()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    \n",
    "    states, actions, next_states, rewards, dones = memory.sample(BATCH_SIZE)\n",
    "    states, actions, next_states, rewards, dones = states.to(device), actions.to(device), next_states.to(device), rewards.to(device), dones.to(device)\n",
    "\n",
    "    # Q(s, a)\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    \n",
    "    # V(s')\n",
    "    next_q_values = target_net(next_states).max(1)[0].detach()\n",
    "    target_q_values = rewards + (GAMMA * next_q_values * (~dones))\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f\"Cella 5: Setup completato su {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af1291ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 6: Funzione train_agent pronta.\n"
     ]
    }
   ],
   "source": [
    "def train_agent(num_episodes):\n",
    "    print(f\"Inizio training per {num_episodes} episodi...\")\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            action = select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            memory.push(state, action, next_state, reward, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            optimize_model()\n",
    "            \n",
    "        if i_episode % TARGET_UPDATE == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "            \n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"Episodio {i_episode}, Reward Totale: {total_reward:.1f}, Epsilon: {EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY):.2f}\")\n",
    "\n",
    "    print(\"Fine training.\")\n",
    "    torch.save(policy_net.state_dict(), \"dama_dqn_final.pth\")\n",
    "\n",
    "print(\"Cella 6: Funzione train_agent pronta.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a03a01b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INIZIO ADDESTRAMENTO ---\n",
      "Inizio training per 100000 episodi...\n",
      "Episodio 0, Reward Totale: -10.0, Epsilon: 0.92\n",
      "Episodio 100, Reward Totale: -10.0, Epsilon: 0.91\n",
      "Episodio 200, Reward Totale: -10.0, Epsilon: 0.90\n",
      "Episodio 300, Reward Totale: -10.0, Epsilon: 0.89\n",
      "Episodio 400, Reward Totale: -10.0, Epsilon: 0.88\n",
      "Episodio 500, Reward Totale: -10.0, Epsilon: 0.87\n",
      "Episodio 600, Reward Totale: -10.0, Epsilon: 0.86\n",
      "Episodio 700, Reward Totale: -10.0, Epsilon: 0.85\n",
      "Episodio 800, Reward Totale: -10.0, Epsilon: 0.84\n",
      "Episodio 900, Reward Totale: -10.0, Epsilon: 0.84\n",
      "Episodio 1000, Reward Totale: -10.0, Epsilon: 0.83\n",
      "Episodio 1100, Reward Totale: -10.0, Epsilon: 0.82\n",
      "Episodio 1200, Reward Totale: -10.0, Epsilon: 0.81\n",
      "Episodio 1300, Reward Totale: -10.0, Epsilon: 0.80\n",
      "Episodio 1400, Reward Totale: -10.0, Epsilon: 0.79\n",
      "Episodio 1500, Reward Totale: -10.0, Epsilon: 0.78\n",
      "Episodio 1600, Reward Totale: -10.0, Epsilon: 0.77\n",
      "Episodio 1700, Reward Totale: -10.0, Epsilon: 0.76\n",
      "Episodio 1800, Reward Totale: -10.0, Epsilon: 0.75\n",
      "Episodio 1900, Reward Totale: -10.0, Epsilon: 0.75\n",
      "Episodio 2000, Reward Totale: -10.0, Epsilon: 0.74\n",
      "Episodio 2100, Reward Totale: -10.0, Epsilon: 0.73\n",
      "Episodio 2200, Reward Totale: -10.0, Epsilon: 0.72\n",
      "Episodio 2300, Reward Totale: -10.0, Epsilon: 0.71\n",
      "Episodio 2400, Reward Totale: -10.0, Epsilon: 0.70\n",
      "Episodio 2500, Reward Totale: -10.0, Epsilon: 0.70\n",
      "Episodio 2600, Reward Totale: -10.0, Epsilon: 0.69\n",
      "Episodio 2700, Reward Totale: -10.0, Epsilon: 0.68\n",
      "Episodio 2800, Reward Totale: -10.0, Epsilon: 0.67\n",
      "Episodio 2900, Reward Totale: -10.0, Epsilon: 0.66\n",
      "Episodio 3000, Reward Totale: -10.0, Epsilon: 0.66\n",
      "Episodio 3100, Reward Totale: -10.0, Epsilon: 0.65\n",
      "Episodio 3200, Reward Totale: -10.0, Epsilon: 0.64\n",
      "Episodio 3300, Reward Totale: -10.0, Epsilon: 0.63\n",
      "Episodio 3400, Reward Totale: -10.0, Epsilon: 0.62\n",
      "Episodio 3500, Reward Totale: -10.0, Epsilon: 0.62\n",
      "Episodio 3600, Reward Totale: -10.0, Epsilon: 0.61\n",
      "Episodio 3700, Reward Totale: -10.0, Epsilon: 0.60\n",
      "Episodio 3800, Reward Totale: -10.0, Epsilon: 0.59\n",
      "Episodio 3900, Reward Totale: -10.0, Epsilon: 0.58\n",
      "Episodio 4000, Reward Totale: -10.0, Epsilon: 0.58\n",
      "Episodio 4100, Reward Totale: -10.0, Epsilon: 0.57\n",
      "Episodio 4200, Reward Totale: -10.0, Epsilon: 0.56\n",
      "Episodio 4300, Reward Totale: -10.0, Epsilon: 0.56\n",
      "Episodio 4400, Reward Totale: -10.0, Epsilon: 0.55\n",
      "Episodio 4500, Reward Totale: -10.0, Epsilon: 0.54\n",
      "Episodio 4600, Reward Totale: -10.0, Epsilon: 0.54\n",
      "Episodio 4700, Reward Totale: -10.0, Epsilon: 0.53\n",
      "Episodio 4800, Reward Totale: -10.0, Epsilon: 0.52\n",
      "Episodio 4900, Reward Totale: -10.0, Epsilon: 0.51\n",
      "Episodio 5000, Reward Totale: -10.0, Epsilon: 0.51\n",
      "Episodio 5100, Reward Totale: -10.0, Epsilon: 0.50\n",
      "Episodio 5200, Reward Totale: -10.0, Epsilon: 0.50\n",
      "Episodio 5300, Reward Totale: -10.0, Epsilon: 0.49\n",
      "Episodio 5400, Reward Totale: -10.0, Epsilon: 0.48\n",
      "Episodio 5500, Reward Totale: -10.0, Epsilon: 0.48\n",
      "Episodio 5600, Reward Totale: -10.0, Epsilon: 0.47\n",
      "Episodio 5700, Reward Totale: -10.0, Epsilon: 0.47\n",
      "Episodio 5800, Reward Totale: -10.0, Epsilon: 0.46\n",
      "Episodio 5900, Reward Totale: -10.0, Epsilon: 0.45\n",
      "Episodio 6000, Reward Totale: -10.0, Epsilon: 0.45\n",
      "Episodio 6100, Reward Totale: -10.0, Epsilon: 0.44\n",
      "Episodio 6200, Reward Totale: -10.0, Epsilon: 0.44\n",
      "Episodio 6300, Reward Totale: -10.0, Epsilon: 0.43\n",
      "Episodio 6400, Reward Totale: -10.0, Epsilon: 0.42\n",
      "Episodio 6500, Reward Totale: -10.0, Epsilon: 0.42\n",
      "Episodio 6600, Reward Totale: -10.0, Epsilon: 0.41\n",
      "Episodio 6700, Reward Totale: -10.0, Epsilon: 0.41\n",
      "Episodio 6800, Reward Totale: -10.0, Epsilon: 0.40\n",
      "Episodio 6900, Reward Totale: -10.0, Epsilon: 0.39\n",
      "Episodio 7000, Reward Totale: -10.0, Epsilon: 0.39\n",
      "Episodio 7100, Reward Totale: -10.0, Epsilon: 0.38\n",
      "Episodio 7200, Reward Totale: -10.0, Epsilon: 0.38\n",
      "Episodio 7300, Reward Totale: -10.0, Epsilon: 0.37\n",
      "Episodio 7400, Reward Totale: -10.0, Epsilon: 0.37\n",
      "Episodio 7500, Reward Totale: -10.0, Epsilon: 0.36\n",
      "Episodio 7600, Reward Totale: -10.0, Epsilon: 0.36\n",
      "Episodio 7700, Reward Totale: -10.0, Epsilon: 0.35\n",
      "Episodio 7800, Reward Totale: -10.0, Epsilon: 0.35\n",
      "Episodio 7900, Reward Totale: -10.0, Epsilon: 0.34\n",
      "Episodio 8000, Reward Totale: -10.0, Epsilon: 0.34\n",
      "Episodio 8100, Reward Totale: -10.0, Epsilon: 0.33\n",
      "Episodio 8200, Reward Totale: -10.0, Epsilon: 0.33\n",
      "Episodio 8300, Reward Totale: -10.0, Epsilon: 0.32\n",
      "Episodio 8400, Reward Totale: -10.0, Epsilon: 0.32\n",
      "Episodio 8500, Reward Totale: -10.0, Epsilon: 0.31\n",
      "Episodio 8600, Reward Totale: -10.0, Epsilon: 0.31\n",
      "Episodio 8700, Reward Totale: -10.0, Epsilon: 0.31\n",
      "Episodio 8800, Reward Totale: -10.0, Epsilon: 0.30\n",
      "Episodio 8900, Reward Totale: -10.0, Epsilon: 0.30\n",
      "Episodio 9000, Reward Totale: -10.0, Epsilon: 0.29\n",
      "Episodio 9100, Reward Totale: -10.0, Epsilon: 0.29\n",
      "Episodio 9200, Reward Totale: -10.0, Epsilon: 0.28\n",
      "Episodio 9300, Reward Totale: -10.0, Epsilon: 0.28\n",
      "Episodio 9400, Reward Totale: -10.0, Epsilon: 0.27\n",
      "Episodio 9500, Reward Totale: -10.0, Epsilon: 0.27\n",
      "Episodio 9600, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 9700, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 9800, Reward Totale: -10.0, Epsilon: 0.26\n",
      "Episodio 9900, Reward Totale: -10.0, Epsilon: 0.25\n",
      "Episodio 10000, Reward Totale: -10.0, Epsilon: 0.25\n",
      "Episodio 10100, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 10200, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 10300, Reward Totale: -10.0, Epsilon: 0.24\n",
      "Episodio 10400, Reward Totale: -10.0, Epsilon: 0.23\n",
      "Episodio 10500, Reward Totale: -10.0, Epsilon: 0.23\n",
      "Episodio 10600, Reward Totale: -10.0, Epsilon: 0.23\n",
      "Episodio 10700, Reward Totale: -10.0, Epsilon: 0.22\n",
      "Episodio 10800, Reward Totale: -10.0, Epsilon: 0.22\n",
      "Episodio 10900, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 11000, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 11100, Reward Totale: -10.0, Epsilon: 0.21\n",
      "Episodio 11200, Reward Totale: -10.0, Epsilon: 0.20\n",
      "Episodio 11300, Reward Totale: -10.0, Epsilon: 0.20\n",
      "Episodio 11400, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 11500, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 11600, Reward Totale: -10.0, Epsilon: 0.19\n",
      "Episodio 11700, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 11800, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 11900, Reward Totale: -10.0, Epsilon: 0.18\n",
      "Episodio 12000, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 12100, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 12200, Reward Totale: -10.0, Epsilon: 0.17\n",
      "Episodio 12300, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 12400, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 12500, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 12600, Reward Totale: -10.0, Epsilon: 0.16\n",
      "Episodio 12700, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 12800, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 12900, Reward Totale: -10.0, Epsilon: 0.15\n",
      "Episodio 13000, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13100, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13200, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13300, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13400, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13500, Reward Totale: -10.0, Epsilon: 0.14\n",
      "Episodio 13600, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 13700, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 13800, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 13900, Reward Totale: -10.0, Epsilon: 0.13\n",
      "Episodio 14000, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 14100, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 14200, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 14300, Reward Totale: -10.0, Epsilon: 0.12\n",
      "Episodio 14400, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 14500, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 14600, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 14700, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 14800, Reward Totale: -10.0, Epsilon: 0.11\n",
      "Episodio 14900, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 15000, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 15100, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 15200, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 15300, Reward Totale: -10.0, Epsilon: 0.10\n",
      "Episodio 15400, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 15500, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 15600, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 15700, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 15800, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 15900, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 16000, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 16100, Reward Totale: -10.0, Epsilon: 0.09\n",
      "Episodio 16200, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16300, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16400, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16500, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16600, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16700, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16800, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 16900, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17000, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17100, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17200, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17300, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17400, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17500, Reward Totale: -10.0, Epsilon: 0.08\n",
      "Episodio 17600, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 17700, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 17800, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 17900, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18000, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18100, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18200, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18300, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18400, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18500, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18600, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18700, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18800, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 18900, Reward Totale: -10.0, Epsilon: 0.07\n",
      "Episodio 19000, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19100, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19200, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19300, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19400, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19500, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19600, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19700, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19800, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 19900, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 20000, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 20100, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 20200, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 20300, Reward Totale: -10.0, Epsilon: 0.06\n",
      "Episodio 20400, Reward Totale: -10.0, Epsilon: 0.06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 1. ADDESTRAMENTO\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Aumentiamo un po' gli episodi per dargli una chance di imparare qualcosa\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- INIZIO ADDESTRAMENTO ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100000\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# 2. TEST PARTITA (Con Mascheramento delle azioni illegali)\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- TEST PARTITA (AI vs AI) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_agent\u001b[39m\u001b[34m(num_episodes)\u001b[39m\n\u001b[32m     15\u001b[39m     state = next_state\n\u001b[32m     16\u001b[39m     total_reward += reward\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i_episode % TARGET_UPDATE == \u001b[32m0\u001b[39m:\n\u001b[32m     21\u001b[39m     target_net.load_state_dict(policy_net.state_dict())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36moptimize_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m optimizer.zero_grad()\n\u001b[32m     54\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\optimizer.py:517\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    512\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    513\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    514\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    515\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    520\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\optimizer.py:82\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     84\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\optimizer.py:150\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\adam.py:953\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    950\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    951\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m953\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    961\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    962\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    963\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    964\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    965\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\torch\\optim\\adam.py:447\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    444\u001b[39m     device_beta1 = beta1\n\u001b[32m    446\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m447\u001b[39m \u001b[43mexp_avg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(beta2, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# MAIN - AVVIO DEL PROGRAMMA CON ACTION MASKING\n",
    "# ==============================================================================\n",
    "\n",
    "# Funzione per convertire una mossa (r1, c1, r2, c2) in indice azione (0-4095)\n",
    "def encode_action(r1, c1, r2, c2):\n",
    "    return r1 * (DIMENSIONE**3) + c1 * (DIMENSIONE**2) + r2 * DIMENSIONE + c2\n",
    "\n",
    "# 1. ADDESTRAMENTO\n",
    "# Aumentiamo un po' gli episodi per dargli una chance di imparare qualcosa\n",
    "print(\"--- INIZIO ADDESTRAMENTO ---\")\n",
    "train_agent(num_episodes=100000) \n",
    "\n",
    "# 2. TEST PARTITA (Con Mascheramento delle azioni illegali)\n",
    "print(\"\\n--- TEST PARTITA (AI vs AI) ---\")\n",
    "obs, info = env.reset()\n",
    "env.render()\n",
    "\n",
    "step_count = 0\n",
    "max_steps = 50 # Limitiamo la partita a 50 mosse per evitare loop infiniti\n",
    "\n",
    "for _ in range(max_steps):\n",
    "    step_count += 1\n",
    "    \n",
    "    # 1. Ottieni le previsioni dalla rete per TUTTE le azioni\n",
    "    state_tensor = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = policy_net(state_tensor).cpu().numpy().flatten() # Vettore di 4096 valori\n",
    "    \n",
    "    # 2. Ottieni le mosse legali dall'ambiente\n",
    "    legal_moves_tuples = env.possible_moves # Lista di (r1, c1, r2, c2)\n",
    "    \n",
    "    if not legal_moves_tuples:\n",
    "        print(\"Nessuna mossa legale disponibile. Partita terminata.\")\n",
    "        break\n",
    "        \n",
    "    # 3. ACTION MASKING: Scegli la mossa legale con il Q-value più alto\n",
    "    best_legal_action_idx = -1\n",
    "    best_legal_q_value = -float('inf')\n",
    "    \n",
    "    for move in legal_moves_tuples:\n",
    "        # Converti la mossa legale nel suo indice (0-4095)\n",
    "        r1, c1, r2, c2 = move\n",
    "        idx = encode_action(r1, c1, r2, c2)\n",
    "        \n",
    "        # Controlla se questa mossa ha un valore più alto delle altre legali trovate finora\n",
    "        if q_values[idx] > best_legal_q_value:\n",
    "            best_legal_q_value = q_values[idx]\n",
    "            best_legal_action_idx = idx\n",
    "            \n",
    "    # Se per qualche motivo non trova nulla (impossibile se la lista non è vuota), prendi la prima a caso\n",
    "    if best_legal_action_idx == -1:\n",
    "        r1, c1, r2, c2 = legal_moves_tuples[0]\n",
    "        best_legal_action_idx = encode_action(r1, c1, r2, c2)\n",
    "\n",
    "    # 4. Esegui l'azione SCELTA TRA QUELLE LEGALI\n",
    "    obs, reward, terminated, truncated, info = env.step(best_legal_action_idx)\n",
    "    env.render()\n",
    "    print(f\"Step: {step_count}, Reward: {reward}\")\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        winner = \"NERO\" if reward > 0 else \"BIANCO\" # Semplificazione basata sull'ultimo reward\n",
    "        print(f\"Fine partita. Risultato finale: {reward}\")\n",
    "        break\n",
    "\n",
    "print(\"Test completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4183d148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello caricato con successo!\n",
      "ROBOT: Sposta la pedina da (5, 0) a (3, 2)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Carichiamo il cervello addestrato\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DQN((8, 8), 4096).to(device)\n",
    "\n",
    "# Se hai già fatto il training lungo, carica i pesi:\n",
    "try:\n",
    "    model.load_state_dict(torch.load(\"dama_dqn_final.pth\", map_location=device))\n",
    "    model.eval() # Imposta in modalità valutazione (niente apprendimento)\n",
    "    print(\"Modello caricato con successo!\")\n",
    "except:\n",
    "    print(\"Attenzione: File del modello non trovato. L'IA giocherà a caso.\")\n",
    "\n",
    "def get_best_move_for_robot(board_matrix, current_player_color):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        board_matrix: numpy array 8x8 (0=vuoto, 1=nero, etc...)\n",
    "        current_player_color: 1 (Nero) o 3 (Bianco)\n",
    "    Output:\n",
    "        ((r1, c1), (r2, c2)): Coordinate di partenza e arrivo\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Creiamo un ambiente temporaneo per calcolare le mosse legali\n",
    "    temp_env = CheckersEnv()\n",
    "    temp_env.board = board_matrix.copy()\n",
    "    temp_env.current_player = current_player_color\n",
    "    legal_moves = temp_env._find_all_legal_moves()\n",
    "    \n",
    "    if not legal_moves:\n",
    "        return None # Nessuna mossa possibile (Perso)\n",
    "\n",
    "    # 2. Chiediamo alla Rete Neurale\n",
    "    state_tensor = torch.tensor(board_matrix, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        q_values = model(state_tensor).cpu().numpy().flatten()\n",
    "\n",
    "    # 3. Action Masking (Scegli la migliore tra le legali)\n",
    "    best_move = None\n",
    "    best_val = -float('inf')\n",
    "\n",
    "    for move in legal_moves:\n",
    "        r1, c1, r2, c2 = move\n",
    "        # Formula inversa per ottenere l'indice da (r,c)\n",
    "        idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "        \n",
    "        if q_values[idx] > best_val:\n",
    "            best_val = q_values[idx]\n",
    "            best_move = ((r1, c1), (r2, c2))\n",
    "            \n",
    "    return best_move\n",
    "\n",
    "# --- ESEMPIO DI UTILIZZO ---\n",
    "# Immagina che la telecamera veda questa scacchiera:\n",
    "fake_camera_board = np.zeros((8,8), dtype=int)\n",
    "fake_camera_board[5, 0] = 3 # Pedina bianca in basso sinistra\n",
    "fake_camera_board[4, 1] = 1 # Pedina nera vicina (da mangiare)\n",
    "\n",
    "# Chiediamo all'IA cosa fare (Gioca il BIANCO = 3)\n",
    "mossa = get_best_move_for_robot(fake_camera_board, current_player_color=3)\n",
    "\n",
    "if mossa:\n",
    "    start, end = mossa\n",
    "    print(f\"ROBOT: Sposta la pedina da {start} a {end}\")\n",
    "else:\n",
    "    print(\"ROBOT: Non ho mosse, ho perso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79dd8afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modello AI caricato con successo!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. CONFIGURAZIONI E COSTANTI\n",
    "# ==============================================================================\n",
    "VUOTO = 0\n",
    "NERO_PEDINA = 1\n",
    "NERO_DAMA = 2\n",
    "BIANCO_PEDINA = 3\n",
    "BIANCO_DAMA = 4\n",
    "DIMENSIONE = 8\n",
    "SQUARE_SIZE = 80 # Grandezza caselle in pixel\n",
    "WIDTH, HEIGHT = DIMENSIONE * SQUARE_SIZE, DIMENSIONE * SQUARE_SIZE\n",
    "\n",
    "# Colori\n",
    "RED = (200, 50, 50)\n",
    "WHITE = (255, 255, 255)\n",
    "BLACK = (0, 0, 0)\n",
    "BEIGE = (210, 180, 140)\n",
    "BROWN = (139, 69, 19)\n",
    "BLUE = (50, 50, 200)\n",
    "GREEN = (0, 255, 0)\n",
    "GOLD = (255, 215, 0)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. CLASSE AMBIENTE (CheckersEnv) - LOGICA COMPLETA\n",
    "# ==============================================================================\n",
    "class CheckersEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(DIMENSIONE**4)\n",
    "        self.observation_space = spaces.Box(low=VUOTO, high=BIANCO_DAMA, shape=(DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        \n",
    "        self.player_map = {NERO_PEDINA: NERO_DAMA, BIANCO_PEDINA: BIANCO_DAMA}\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        board = np.zeros((DIMENSIONE, DIMENSIONE), dtype=np.int32)\n",
    "        # NERO (1)\n",
    "        for r in range(3):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: board[r, c] = NERO_PEDINA\n",
    "        # BIANCO (3)\n",
    "        for r in range(5, 8):\n",
    "            for c in range(DIMENSIONE):\n",
    "                if (r + c) % 2 == 1: board[r, c] = BIANCO_PEDINA\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = NERO_PEDINA \n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {'current_player': self.current_player}\n",
    "\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // (DIMENSIONE**3)\n",
    "        c1 = (action_int // (DIMENSIONE**2)) % DIMENSIONE\n",
    "        r2 = (action_int // DIMENSIONE) % DIMENSIONE\n",
    "        c2 = action_int % DIMENSIONE\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    # --- LOGICA MOVIMENTO MANCANTE NEL TUO CODICE PRECEDENTE ---\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal_moves = []\n",
    "        king_val = self.player_map.get(self.current_player, 0)\n",
    "        \n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = self.board[r, c]\n",
    "                if piece == self.current_player or piece == king_val:\n",
    "                    captures.extend(self._get_piece_captures(r, c))\n",
    "                    if not captures:\n",
    "                        normal_moves.extend(self._get_piece_moves(r, c))\n",
    "        \n",
    "        return captures if captures else normal_moves\n",
    "\n",
    "    def _get_piece_moves(self, r, c):\n",
    "        moves = []\n",
    "        piece = self.board[r, c]\n",
    "        if piece == NERO_PEDINA: dirs = [(1, -1), (1, 1)]\n",
    "        elif piece == BIANCO_PEDINA: dirs = [(-1, -1), (-1, 1)]\n",
    "        else: dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)]\n",
    "\n",
    "        for dr, dc in dirs:\n",
    "            nr, nc = r + dr, c + dc\n",
    "            if 0 <= nr < DIMENSIONE and 0 <= nc < DIMENSIONE:\n",
    "                if self.board[nr, nc] == VUOTO:\n",
    "                    moves.append((r, c, nr, nc))\n",
    "        return moves\n",
    "\n",
    "    def _get_piece_captures(self, r, c):\n",
    "        captures = []\n",
    "        piece = self.board[r, c]\n",
    "        \n",
    "        if piece in (NERO_PEDINA, NERO_DAMA):\n",
    "            enemies = (BIANCO_PEDINA, BIANCO_DAMA)\n",
    "            pawn_dirs = [(1, -1), (1, 1)]\n",
    "        else:\n",
    "            enemies = (NERO_PEDINA, NERO_DAMA)\n",
    "            pawn_dirs = [(-1, -1), (-1, 1)]\n",
    "            \n",
    "        is_king = piece in (NERO_DAMA, BIANCO_DAMA)\n",
    "        dirs = [(-1, -1), (-1, 1), (1, -1), (1, 1)] if is_king else pawn_dirs\n",
    "        \n",
    "        for dr, dc in dirs:\n",
    "            mid_r, mid_c = r + dr, c + dc\n",
    "            land_r, land_c = r + 2*dr, c + 2*dc\n",
    "            \n",
    "            if 0 <= land_r < DIMENSIONE and 0 <= land_c < DIMENSIONE:\n",
    "                if self.board[mid_r, mid_c] in enemies:\n",
    "                    if self.board[land_r, land_c] == VUOTO:\n",
    "                        captures.append((r, c, land_r, land_c))\n",
    "        return captures\n",
    "\n",
    "    def _execute_move(self, r1, c1, r2, c2):\n",
    "        piece = self.board[r1, c1]\n",
    "        self.board[r2, c2] = piece\n",
    "        self.board[r1, c1] = VUOTO\n",
    "        reward = 0\n",
    "        \n",
    "        # Cattura\n",
    "        if abs(r2 - r1) == 2:\n",
    "            self.board[(r1 + r2) // 2, (c1 + c2) // 2] = VUOTO\n",
    "            reward += 1.0\n",
    "            \n",
    "        # Promozione\n",
    "        if piece == NERO_PEDINA and r2 == 7:\n",
    "            self.board[r2, c2] = NERO_DAMA\n",
    "            reward += 3.0\n",
    "        elif piece == BIANCO_PEDINA and r2 == 0:\n",
    "            self.board[r2, c2] = BIANCO_DAMA\n",
    "            reward += 3.0\n",
    "            \n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        r1, c1, r2, c2 = self._decode_action(action)\n",
    "        move = (r1, c1, r2, c2)\n",
    "        terminated = False\n",
    "        reward = 0\n",
    "        \n",
    "        if move in self.possible_moves:\n",
    "            reward += self._execute_move(r1, c1, r2, c2)\n",
    "            # Cambio turno\n",
    "            self.current_player = BIANCO_PEDINA if self.current_player == NERO_PEDINA else NERO_PEDINA\n",
    "            \n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            if not self.possible_moves:\n",
    "                terminated = True\n",
    "                reward += 10.0 \n",
    "        else:\n",
    "            reward = -10.0 \n",
    "            terminated = True \n",
    "\n",
    "        return self.board.copy(), reward, terminated, False, {}\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. RETE NEURALE (DQN)\n",
    "# ==============================================================================\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        conv_out_size = 64 * input_shape[0] * input_shape[1]\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512), nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1).float()\n",
    "        return self.fc(self.conv(x))\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. INTERFACCIA GRAFICA (PYGAME)\n",
    "# ==============================================================================\n",
    "def get_row_col_from_mouse(pos):\n",
    "    x, y = pos\n",
    "    row = y // SQUARE_SIZE\n",
    "    col = x // SQUARE_SIZE\n",
    "    return row, col\n",
    "\n",
    "def main_gui():\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption('Dama AI vs Human')\n",
    "    clock = pygame.time.Clock()\n",
    "\n",
    "    env = CheckersEnv() \n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy_net = DQN((8, 8), 8**4).to(device)\n",
    "    \n",
    "    try:\n",
    "        policy_net.load_state_dict(torch.load(\"dama_dqn_final.pth\", map_location=device))\n",
    "        policy_net.eval()\n",
    "        print(\"Modello AI caricato con successo!\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"ATTENZIONE: File 'dama_dqn_final.pth' non trovato.\")\n",
    "        print(\"L'IA giocherà mosse casuali (o la prima mossa legale disponibile).\")\n",
    "\n",
    "    selected_piece = None\n",
    "    valid_destinations = [] \n",
    "    \n",
    "    running = True\n",
    "    game_over = False\n",
    "    winner_text = \"\"\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "\n",
    "            # --- TURNO UMANO (NERO) ---\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN and not game_over:\n",
    "                if env.current_player == NERO_PEDINA:\n",
    "                    pos = pygame.mouse.get_pos()\n",
    "                    row, col = get_row_col_from_mouse(pos)\n",
    "                    \n",
    "                    clicked_piece = env.board[row, col]\n",
    "                    \n",
    "                    # 1. Seleziona pedina\n",
    "                    if clicked_piece in (NERO_PEDINA, NERO_DAMA):\n",
    "                        selected_piece = (row, col)\n",
    "                        valid_destinations = []\n",
    "                        for move in env.possible_moves:\n",
    "                            r1, c1, r2, c2 = move\n",
    "                            if (r1, c1) == selected_piece:\n",
    "                                valid_destinations.append((r2, c2))\n",
    "                                \n",
    "                    # 2. Muovi pedina\n",
    "                    elif selected_piece and (row, col) in valid_destinations:\n",
    "                        r1, c1 = selected_piece\n",
    "                        r2, c2 = (row, col)\n",
    "                        action_idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "                        \n",
    "                        obs, reward, terminated, truncated, info = env.step(action_idx)\n",
    "                        selected_piece = None\n",
    "                        valid_destinations = []\n",
    "                        \n",
    "                        if terminated:\n",
    "                            game_over = True\n",
    "                            winner_text = \"HAI VINTO!\"\n",
    "\n",
    "        # --- TURNO AI (BIANCO) ---\n",
    "        if env.current_player == BIANCO_PEDINA and not game_over:\n",
    "            pygame.display.set_caption(\"L'IA sta pensando...\")\n",
    "            pygame.display.flip()\n",
    "            time.sleep(0.5) \n",
    "            \n",
    "            legal_moves = env.possible_moves\n",
    "            if not legal_moves:\n",
    "                game_over = True\n",
    "                winner_text = \"HAI VINTO! (AI bloccata)\"\n",
    "            else:\n",
    "                state_tensor = torch.tensor(env.board, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_values = policy_net(state_tensor).cpu().numpy().flatten()\n",
    "                \n",
    "                best_action = -1\n",
    "                best_val = -float('inf')\n",
    "                \n",
    "                # Action Masking\n",
    "                found_move = False\n",
    "                for move in legal_moves:\n",
    "                    r1, c1, r2, c2 = move\n",
    "                    idx = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "                    if q_values[idx] > best_val:\n",
    "                        best_val = q_values[idx]\n",
    "                        best_action = idx\n",
    "                        found_move = True\n",
    "                \n",
    "                # Fallback se qualcosa va storto\n",
    "                if not found_move:\n",
    "                    r1, c1, r2, c2 = legal_moves[0]\n",
    "                    best_action = r1 * 512 + c1 * 64 + r2 * 8 + c2\n",
    "\n",
    "                obs, reward, terminated, truncated, info = env.step(best_action)\n",
    "                \n",
    "                if terminated:\n",
    "                    game_over = True\n",
    "                    winner_text = \"HA VINTO L'IA!\"\n",
    "            \n",
    "            pygame.display.set_caption('Dama AI vs Human')\n",
    "\n",
    "        # --- DISEGNO ---\n",
    "        screen.fill(BLACK)\n",
    "        \n",
    "        # Scacchiera\n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                color = BEIGE if (r + c) % 2 == 0 else BROWN\n",
    "                pygame.draw.rect(screen, color, (c*SQUARE_SIZE, r*SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE))\n",
    "                if (r, c) in valid_destinations:\n",
    "                    pygame.draw.circle(screen, GREEN, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), 10)\n",
    "\n",
    "        # Pedine\n",
    "        for r in range(DIMENSIONE):\n",
    "            for c in range(DIMENSIONE):\n",
    "                piece = env.board[r, c]\n",
    "                if piece != 0:\n",
    "                    color = RED if piece in (1, 2) else WHITE\n",
    "                    pygame.draw.circle(screen, color, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), SQUARE_SIZE//2 - 10)\n",
    "                    if piece in (2, 4):\n",
    "                        pygame.draw.circle(screen, GOLD, (c*SQUARE_SIZE + SQUARE_SIZE//2, r*SQUARE_SIZE + SQUARE_SIZE//2), SQUARE_SIZE//2 - 25, 3)\n",
    "                    if selected_piece == (r, c):\n",
    "                        pygame.draw.rect(screen, GREEN, (c*SQUARE_SIZE, r*SQUARE_SIZE, SQUARE_SIZE, SQUARE_SIZE), 3)\n",
    "\n",
    "        if game_over:\n",
    "            font = pygame.font.SysFont(None, 75)\n",
    "            text_surf = font.render(winner_text, True, BLUE)\n",
    "            # Sfondo per il testo\n",
    "            text_rect = text_surf.get_rect(center=(WIDTH//2, HEIGHT//2))\n",
    "            pygame.draw.rect(screen, WHITE, text_rect.inflate(20, 20)) \n",
    "            screen.blit(text_surf, text_rect)\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(60)\n",
    "\n",
    "    pygame.quit()\n",
    "    sys.exit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main_gui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb338642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cella 1 completata: Classi definite.\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "import pygame\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# --- CONFIGURAZIONI ---\n",
    "DIMENSIONE = 8\n",
    "ACTION_DIM = DIMENSIONE**4\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- AMBIENTE ---\n",
    "class CheckersEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(CheckersEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(ACTION_DIM)\n",
    "        self.observation_space = spaces.Box(low=0, high=4, shape=(8,8), dtype=np.int32)\n",
    "        self.player_map = {1: 2, 3: 4}\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = []\n",
    "\n",
    "    def _initialize_board(self):\n",
    "        board = np.zeros((8, 8), dtype=np.int32)\n",
    "        for r in range(3):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 1\n",
    "        for r in range(5, 8):\n",
    "            for c in range(8):\n",
    "                if (r + c) % 2 == 1: board[r, c] = 3\n",
    "        return board\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        self.board = self._initialize_board()\n",
    "        self.current_player = 1\n",
    "        self.possible_moves = self._find_all_legal_moves()\n",
    "        return self.board.copy(), {}\n",
    "\n",
    "    def _decode_action(self, action_int):\n",
    "        r1 = action_int // 512\n",
    "        c1 = (action_int // 64) % 8\n",
    "        r2 = (action_int // 8) % 8\n",
    "        c2 = action_int % 8\n",
    "        return r1, c1, r2, c2\n",
    "\n",
    "    def _find_all_legal_moves(self):\n",
    "        captures = []\n",
    "        normal = []\n",
    "        king = self.player_map.get(self.current_player, 0)\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                if self.board[r,c] == self.current_player or self.board[r,c] == king:\n",
    "                    captures.extend(self._get_captures(r, c))\n",
    "                    if not captures: normal.extend(self._get_moves(r, c))\n",
    "        return captures if captures else normal\n",
    "\n",
    "    def _get_moves(self, r, c):\n",
    "        m = []\n",
    "        dirs = [(1,-1),(1,1)] if self.board[r,c]==1 else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs = [(1,-1),(1,1),(-1,-1),(-1,1)]\n",
    "        for dr, dc in dirs:\n",
    "            if 0<=r+dr<8 and 0<=c+dc<8 and self.board[r+dr,c+dc]==0:\n",
    "                m.append((r,c,r+dr,c+dc))\n",
    "        return m\n",
    "\n",
    "    def _get_captures(self, r, c):\n",
    "        caps = []\n",
    "        enemies = [3,4] if self.board[r,c] in [1,2] else [1,2]\n",
    "        dirs = [(1,-1),(1,1)] if self.board[r,c] in [1,2] else [(-1,-1),(-1,1)]\n",
    "        if self.board[r,c] in [2,4]: dirs = [(1,-1),(1,1),(-1,-1),(-1,1)]\n",
    "        for dr, dc in dirs:\n",
    "            if 0<=r+2*dr<8 and 0<=c+2*dc<8:\n",
    "                if self.board[r+dr,c+dc] in enemies and self.board[r+2*dr,c+2*dc]==0:\n",
    "                    caps.append((r,c,r+2*dr,c+2*dc))\n",
    "        return caps\n",
    "\n",
    "    def step(self, action):\n",
    "        r1,c1,r2,c2 = self._decode_action(action)\n",
    "        if (r1,c1,r2,c2) in self.possible_moves:\n",
    "            self.board[r2,c2] = self.board[r1,c1]\n",
    "            self.board[r1,c1] = 0\n",
    "            rew = 0\n",
    "            if abs(r2-r1) == 2: \n",
    "                self.board[(r1+r2)//2, (c1+c2)//2] = 0\n",
    "                rew = 1.0\n",
    "            if (self.board[r2,c2]==1 and r2==7) or (self.board[r2,c2]==3 and r2==0):\n",
    "                self.board[r2,c2] += 1\n",
    "                rew += 3.0\n",
    "            \n",
    "            self.current_player = 3 if self.current_player == 1 else 1\n",
    "            self.possible_moves = self._find_all_legal_moves()\n",
    "            \n",
    "            done = not self.possible_moves\n",
    "            if done: rew += 10.0\n",
    "            \n",
    "            return self.board.copy(), rew, done, False, {}\n",
    "        else:\n",
    "            return self.board.copy(), -10.0, True, False, {}\n",
    "\n",
    "# --- AI ---\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64*64, 512), nn.ReLU(),\n",
    "            nn.Linear(512, ACTION_DIM)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x.unsqueeze(1).float())\n",
    "\n",
    "print(\"Cella 1 completata: Classi definite.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f142a450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INIZIO ADDESTRAMENTO DOPPIO ---\n",
      "Obiettivo 1: 1.000 episodi -> 'dama_1k.pth'\n",
      "Obiettivo 2: 10.000 episodi -> 'dama_10k.pth'\n",
      "Episodio 500 completato. Epsilon: 1.00\n",
      "Episodio 1000 completato. Epsilon: 0.99\n",
      ">>> SALVATO MODELLO 1k: dama_1k.pth\n",
      "Episodio 1500 completato. Epsilon: 0.99\n",
      "Episodio 2000 completato. Epsilon: 0.98\n",
      "Episodio 2500 completato. Epsilon: 0.98\n",
      "Episodio 3000 completato. Epsilon: 0.97\n",
      "Episodio 3500 completato. Epsilon: 0.97\n",
      "Episodio 4000 completato. Epsilon: 0.96\n",
      "Episodio 4500 completato. Epsilon: 0.96\n",
      "Episodio 5000 completato. Epsilon: 0.95\n",
      "Episodio 5500 completato. Epsilon: 0.95\n",
      "Episodio 6000 completato. Epsilon: 0.94\n",
      "Episodio 6500 completato. Epsilon: 0.94\n",
      "Episodio 7000 completato. Epsilon: 0.93\n",
      "Episodio 7500 completato. Epsilon: 0.93\n",
      "Episodio 8000 completato. Epsilon: 0.92\n",
      "Episodio 8500 completato. Epsilon: 0.92\n",
      "Episodio 9000 completato. Epsilon: 0.92\n",
      "Episodio 9500 completato. Epsilon: 0.91\n",
      "Episodio 10000 completato. Epsilon: 0.91\n",
      ">>> SALVATO MODELLO 10k: dama_10k.pth\n",
      "Addestramento completato.\n"
     ]
    }
   ],
   "source": [
    "# Setup Training\n",
    "env = CheckersEnv()\n",
    "policy_net = DQN().to(DEVICE)\n",
    "target_net = DQN().to(DEVICE)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n",
    "memory = deque(maxlen=20000)\n",
    "\n",
    "steps_done = 0\n",
    "EPS_START, EPS_END, EPS_DECAY = 1.0, 0.05, 100000\n",
    "\n",
    "print(\"--- INIZIO ADDESTRAMENTO DOPPIO ---\")\n",
    "print(\"Obiettivo 1: 1.000 episodi -> 'dama_1k.pth'\")\n",
    "print(\"Obiettivo 2: 10.000 episodi -> 'dama_10k.pth'\")\n",
    "\n",
    "for i_episode in range(1, 10001):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        steps_done += 1\n",
    "        eps = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "        \n",
    "        # Select Action\n",
    "        if random.random() > eps:\n",
    "            with torch.no_grad():\n",
    "                state_t = torch.tensor(state).unsqueeze(0).to(DEVICE)\n",
    "                action = policy_net(state_t).argmax().item()\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Step\n",
    "        next_state, reward, term, trunc, _ = env.step(action)\n",
    "        done = term or trunc\n",
    "        \n",
    "        memory.append((state, action, next_state, reward, done))\n",
    "        state = next_state\n",
    "        \n",
    "        # Optimize\n",
    "        if len(memory) > 64:\n",
    "            batch = random.sample(memory, 64)\n",
    "            states, acts, nexts, rews, dones = zip(*batch)\n",
    "            \n",
    "            S = torch.tensor(np.array(states)).to(DEVICE)\n",
    "            A = torch.tensor(acts).unsqueeze(1).to(DEVICE)\n",
    "            R = torch.tensor(rews).to(DEVICE)\n",
    "            NS = torch.tensor(np.array(nexts)).to(DEVICE)\n",
    "            D = torch.tensor(dones).to(DEVICE)\n",
    "            \n",
    "            Q = policy_net(S).gather(1, A).squeeze()\n",
    "            next_Q = target_net(NS).max(1)[0].detach()\n",
    "            Target = R + 0.99 * next_Q * (~D)\n",
    "            \n",
    "            loss = nn.MSELoss()(Q, Target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "    # Update Target Network\n",
    "    if i_episode % 500 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f\"Episodio {i_episode} completato. Epsilon: {eps:.2f}\")\n",
    "\n",
    "    # SALVATAGGIO STEP 1 (1.000 Episodi)\n",
    "    if i_episode == 1000:\n",
    "        torch.save(policy_net.state_dict(), \"dama_1k.pth\")\n",
    "        print(\">>> SALVATO MODELLO 1k: dama_1k.pth\")\n",
    "\n",
    "    # SALVATAGGIO STEP 2 (10.000 Episodi)\n",
    "    if i_episode == 10000:\n",
    "        torch.save(policy_net.state_dict(), \"dama_10k.pth\")\n",
    "        print(\">>> SALVATO MODELLO 10k: dama_10k.pth\")\n",
    "\n",
    "print(\"Addestramento completato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "75af16cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contro chi vuoi giocare?\n",
      "1 -> Principiante (1k episodi)\n",
      "2 -> Esperto (10k episodi)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'choice' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    141\u001b[39m         clock.tick(\u001b[32m30\u001b[39m)\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# Avvia il gioco\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m \u001b[43mplay_game_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mplay_game_notebook\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m2 -> Esperto (10k episodi)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m scelta = \u001b[38;5;28minput\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInserisci 1 o 2: \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mchoice\u001b[49m == \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     18\u001b[39m     model_path = \u001b[33m\"\u001b[39m\u001b[33mdama_1k.pth\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     19\u001b[39m     title = \u001b[33m\"\u001b[39m\u001b[33mDama vs Principiante (1k)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'choice' is not defined"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURAZIONE GRAFICA ---\n",
    "SQUARE_SIZE = 80\n",
    "WIDTH, HEIGHT = 640, 640\n",
    "COLORS = {\n",
    "    'W': (255,255,255), 'B': (0,0,0), 'R': (200,50,50), \n",
    "    'Beige': (210,180,140), 'Brown': (139,69,19), \n",
    "    'Green': (0,255,0), 'Gold': (255,215,0)\n",
    "}\n",
    "\n",
    "def play_game_notebook():\n",
    "    # INPUT UTENTE\n",
    "    print(\"Contro chi vuoi giocare?\")\n",
    "    print(\"1 -> Principiante (1k episodi)\")\n",
    "    print(\"2 -> Esperto (10k episodi)\")\n",
    "    scelta = input(\"Inserisci 1 o 2: \")\n",
    "    \n",
    "    if choice == '1':\n",
    "        model_path = \"dama_1k.pth\"\n",
    "        title = \"Dama vs Principiante (1k)\"\n",
    "    else:\n",
    "        model_path = \"dama_10k.pth\"\n",
    "        title = \"Dama vs Esperto (10k)\"\n",
    "        \n",
    "    print(f\"Caricamento {model_path}...\")\n",
    "\n",
    "    # Carica Modello\n",
    "    game_net = DQN().to(DEVICE)\n",
    "    try:\n",
    "        game_net.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "        game_net.eval()\n",
    "    except FileNotFoundError:\n",
    "        print(\"Errore: File non trovato. Hai eseguito la Cella 2 fino in fondo?\")\n",
    "        return\n",
    "\n",
    "    # Inizializza Pygame\n",
    "    pygame.init()\n",
    "    screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "    pygame.display.set_caption(title)\n",
    "    clock = pygame.time.Clock()\n",
    "    \n",
    "    env = CheckersEnv()\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    selected = None\n",
    "    running = True\n",
    "    game_over = False\n",
    "    font = pygame.font.SysFont(None, 60)\n",
    "\n",
    "    while running:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                running = False\n",
    "                pygame.quit()\n",
    "                return # Esce dalla funzione per non bloccare il notebook\n",
    "\n",
    "            # TURNO UMANO (Nero/Rosso = 1)\n",
    "            if event.type == pygame.MOUSEBUTTONDOWN and not game_over and env.current_player == 1:\n",
    "                pos = pygame.mouse.get_pos()\n",
    "                r, c = pos[1]//SQUARE_SIZE, pos[0]//SQUARE_SIZE\n",
    "                \n",
    "                # Clicca su pezzo\n",
    "                if env.board[r,c] in [1, 2]:\n",
    "                    selected = (r,c)\n",
    "                # Clicca su destinazione\n",
    "                elif selected:\n",
    "                    r1, c1 = selected\n",
    "                    # Trova se la mossa è valida\n",
    "                    act = r1*512 + c1*64 + r*8 + c\n",
    "                    \n",
    "                    # Cerca se l'azione è nelle mosse possibili\n",
    "                    is_valid = False\n",
    "                    for m in env.possible_moves:\n",
    "                        if m == (r1, c1, r, c):\n",
    "                            is_valid = True\n",
    "                            break\n",
    "                    \n",
    "                    if is_valid:\n",
    "                        obs, _, term, _, _ = env.step(act)\n",
    "                        selected = None\n",
    "                        if term: \n",
    "                            game_over = True\n",
    "                            print(\"HAI VINTO!\")\n",
    "\n",
    "        # TURNO AI (Bianco = 3)\n",
    "        if env.current_player == 3 and not game_over:\n",
    "            pygame.display.flip()\n",
    "            time.sleep(0.5)\n",
    "            \n",
    "            legal = env.possible_moves\n",
    "            if not legal:\n",
    "                game_over = True\n",
    "                print(\"L'IA non ha mosse. HAI VINTO!\")\n",
    "            else:\n",
    "                # Action Masking\n",
    "                with torch.no_grad():\n",
    "                    t_obs = torch.tensor(env.board).unsqueeze(0).to(DEVICE)\n",
    "                    q_vals = game_net(t_obs).cpu().numpy().flatten()\n",
    "                \n",
    "                best_act = -1\n",
    "                best_val = -float('inf')\n",
    "                \n",
    "                for (r1, c1, r2, c2) in legal:\n",
    "                    idx = r1*512 + c1*64 + r2*8 + c2\n",
    "                    if q_vals[idx] > best_val:\n",
    "                        best_val = q_vals[idx]\n",
    "                        best_act = idx\n",
    "                \n",
    "                obs, _, term, _, _ = env.step(best_act)\n",
    "                if term:\n",
    "                    game_over = True\n",
    "                    print(\"L'IA HA VINTO!\")\n",
    "\n",
    "        # DISEGNO\n",
    "        screen.fill(COLORS['B'])\n",
    "        for r in range(8):\n",
    "            for c in range(8):\n",
    "                col = COLORS['Beige'] if (r+c)%2==0 else COLORS['Brown']\n",
    "                pygame.draw.rect(screen, col, (c*80, r*80, 80, 80))\n",
    "                \n",
    "                # Evidenzia destinazioni\n",
    "                if selected:\n",
    "                    sr, sc = selected\n",
    "                    for m in env.possible_moves:\n",
    "                        if m[0] == sr and m[1] == sc:\n",
    "                             pygame.draw.circle(screen, COLORS['Green'], (m[3]*80+40, m[2]*80+40), 10)\n",
    "\n",
    "                p = env.board[r,c]\n",
    "                if p != 0:\n",
    "                    cc = COLORS['R'] if p in [1,2] else COLORS['W']\n",
    "                    pygame.draw.circle(screen, cc, (c*80+40, r*80+40), 30)\n",
    "                    if p in [2,4]:\n",
    "                        pygame.draw.circle(screen, COLORS['Gold'], (c*80+40, r*80+40), 10)\n",
    "                    if selected == (r,c):\n",
    "                         pygame.draw.rect(screen, COLORS['Green'], (c*80, r*80, 80, 80), 3)\n",
    "        \n",
    "        if game_over:\n",
    "            txt = font.render(\"GAME OVER\", True, COLORS['Green'])\n",
    "            screen.blit(txt, (200, 300))\n",
    "\n",
    "        pygame.display.flip()\n",
    "        clock.tick(30)\n",
    "\n",
    "# Avvia il gioco\n",
    "play_game_notebook()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
